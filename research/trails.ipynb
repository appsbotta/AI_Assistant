{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    github_url: str\n",
    "    linkedin_url: str\n",
    "    local_data_file: Path\n",
    "    save_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bot.constants import *\n",
    "from bot.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathlib.WindowsPath"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_path = CONFIG_PATH,\n",
    "            params_path = PARAMS_PATH\n",
    "        ):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.param = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_ingestion_config(self)->DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            github_url=config.github_url,\n",
    "            linkedin_url=config.linkedin_url,\n",
    "            local_data_file=config.local_data_file,\n",
    "            save_dir=config.save_dir\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bot import logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import json\n",
    "import zipfile\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_repos(self):\n",
    "        logger.info(f\"Starting writing data from git hub repos\")\n",
    "        url = self.config.github_url\n",
    "        headers = {\"Authorization\": f\"token {os.getenv('TOKEN')}\"}\n",
    "        repos = requests.get(url, headers=headers).json()\n",
    "        save_dir = self.config.save_dir\n",
    "        with open(os.path.join(save_dir,\"data.json\"),'w',encoding='utf-8') as f:\n",
    "            json.dump(repos,f,indent=4)\n",
    "        logger.info(f\"Completed writing data from git hub repos\")\n",
    "    \n",
    "    def downloadFiles(self):\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            filename , headers = request.urlretrieve(\n",
    "                url = self.config.linkedin_url,\n",
    "                filename = self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n",
    "        else:\n",
    "            logger.info(f\"file already exists of size\")\n",
    "    \n",
    "    def extractZip(self):\n",
    "        unzip_path = self.config.save_dir\n",
    "        os.makedirs(unzip_path,exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file,'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-16 22:07:25,381: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-16 22:07:25,389: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-16 22:07:25,390: INFO: common: created directory at: artifacts]\n",
      "[2025-03-16 22:07:25,390: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-03-16 22:07:25,397: INFO: 1269584388: Starting writing data from git hub repos]\n",
      "[2025-03-16 22:07:25,990: INFO: 1269584388: Completed writing data from git hub repos]\n",
      "[2025-03-16 22:07:28,445: INFO: 1269584388: artifacts/data_ingestion/data.zip download! with following info: \n",
      "Connection: close\n",
      "Content-Length: 109817\n",
      "Cache-Control: max-age=300\n",
      "Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox\n",
      "Content-Type: application/zip\n",
      "ETag: \"d071a2206aed0a699aa67795ff15d0bdc953e8eacfb973ca60d3a8f15f026908\"\n",
      "Strict-Transport-Security: max-age=31536000\n",
      "X-Content-Type-Options: nosniff\n",
      "X-Frame-Options: deny\n",
      "X-XSS-Protection: 1; mode=block\n",
      "X-GitHub-Request-Id: 5F42:1ABD27:2F6BF9:85EC5B:67D6FC5A\n",
      "Accept-Ranges: bytes\n",
      "Date: Sun, 16 Mar 2025 16:37:30 GMT\n",
      "Via: 1.1 varnish\n",
      "X-Served-By: cache-del21745-DEL\n",
      "X-Cache: HIT\n",
      "X-Cache-Hits: 0\n",
      "X-Timer: S1742143050.805925,VS0,VE280\n",
      "Vary: Authorization,Accept-Encoding,Origin\n",
      "Access-Control-Allow-Origin: *\n",
      "Cross-Origin-Resource-Policy: cross-origin\n",
      "X-Fastly-Request-ID: 54951b7a42cea7947dfbe97158f336b363a8ee0d\n",
      "Expires: Sun, 16 Mar 2025 16:42:30 GMT\n",
      "Source-Age: 0\n",
      "\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.get_repos()\n",
    "    data_ingestion.downloadFiles()\n",
    "    data_ingestion.extractZip()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTrasformationConfig:\n",
    "    root_dir: Path\n",
    "    file_dir: Path\n",
    "    save_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bot.constants import *\n",
    "from bot.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_path = CONFIG_PATH,\n",
    "            params_path = PARAMS_PATH\n",
    "        ):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self)->DataTrasformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_trasformation_config = DataTrasformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            file_dir=config.file_dir,\n",
    "            save_dir=config.save_dir,\n",
    "        )\n",
    "\n",
    "        return data_trasformation_config\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Preformatted\n",
    "from bot import logger\n",
    "import json\n",
    "import markdown2\n",
    "import html2text\n",
    "import base64\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[{'id': 915970693, 'node_id': 'R_kgDONpiahQ', 'name': 'AgenticAi', 'full_name': 'appsbotta/AgenticAi', 'private': False, 'owner': {'login': 'appsbotta', 'id': 75985363, 'node_id': 'MDQ6VXNlcjc1OTg1MzYz', 'avatar_url': 'https://avatars.githubusercontent.com/u/75985363?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/appsbotta', 'html_url': 'https://github.com/appsbotta', 'followers_url': 'https://api.github.com/users/appsbotta/followers', 'following_url': 'https://api.github.com/users/appsbotta/following{/other_user}', 'gists_url': 'https://api.github.com/users/appsbotta/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/appsbotta/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/appsbotta/subscriptions', 'organizations_url': 'https://api.github.com/users/appsbotta/orgs', 'repos_url': 'https://api.github.com/users/appsbotta/repos', 'events_url': 'https://api.github.com/users/appsbotta/events{/privacy}', 'received_events_url': 'https://api.github.com/users/appsbotta/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'html_url': 'https://github.com/appsbotta/AgenticAi', 'description': None, 'fork': False, 'url': 'https://api.github.com/repos/appsbotta/AgenticAi', 'forks_url': 'https://api.github.com/repos/appsbotta/AgenticAi/forks', 'keys_url': 'https://api.github.com/repos/appsbotta/AgenticAi/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/appsbotta/AgenticAi/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/appsbotta/AgenticAi/teams', 'hooks_url': 'https://api.github.com/repos/appsbotta/AgenticAi/hooks', 'issue_events_url': 'https://api.github.com/repos/appsbotta/AgenticAi/issues/events{/number}', 'events_url': 'https://api.github.com/repos/appsbotta/AgenticAi/events', 'assignees_url': 'https://api.github.com/repos/appsbotta/AgenticAi/assignees{/user}', 'branches_url': 'https://api.github.com/repos/appsbotta/AgenticAi/branches{/branch}', 'tags_url': 'https://api.github.com/repos/appsbotta/AgenticAi/tags', 'blobs_url': 'https://api.github.com/repos/appsbotta/AgenticAi/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/appsbotta/AgenticAi/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/appsbotta/AgenticAi/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/appsbotta/AgenticAi/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/appsbotta/AgenticAi/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/appsbotta/AgenticAi/languages', 'stargazers_url': 'https://api.github.com/repos/appsbotta/AgenticAi/stargazers', 'contributors_url': 'https://api.github.com/repos/appsbotta/AgenticAi/contributors', 'subscribers_url': 'https://api.github.com/repos/appsbotta/AgenticAi/subscribers', 'subscription_url': 'https://api.github.com/repos/appsbotta/AgenticAi/subscription', 'commits_url': 'https://api.github.com/repos/appsbotta/AgenticAi/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/appsbotta/AgenticAi/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/appsbotta/AgenticAi/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/appsbotta/AgenticAi/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/appsbotta/AgenticAi/contents/{+path}', 'compare_url': 'https://api.github.com/repos/appsbotta/AgenticAi/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/appsbotta/AgenticAi/merges', 'archive_url': 'https://api.github.com/repos/appsbotta/AgenticAi/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/appsbotta/AgenticAi/downloads', 'issues_url': 'https://api.github.com/repos/appsbotta/AgenticAi/issues{/number}', 'pulls_url': 'https://api.github.com/repos/appsbotta/AgenticAi/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/appsbotta/AgenticAi/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/appsbotta/AgenticAi/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/appsbotta/AgenticAi/labels{/name}', 'releases_url': 'https://api.github.com/repos/appsbotta/AgenticAi/releases{/id}', 'deployments_url': 'https://api.github.com/repos/appsbotta/AgenticAi/deployments', 'created_at': '2025-01-13T07:55:43Z', 'updated_at': '2025-03-10T01:52:18Z', 'pushed_at': '2025-03-10T01:52:15Z', 'git_url': 'git://github.com/appsbotta/AgenticAi.git', 'ssh_url': 'git@github.com:appsbotta/AgenticAi.git', 'clone_url': 'https://github.com/appsbotta/AgenticAi.git', 'svn_url': 'https://github.com/appsbotta/AgenticAi', 'homepage': None, 'size': 7, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Python', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': None, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'main', 'permissions': {'admin': True, 'maintain': True, 'push': True, 'triage': True, 'pull': True}}, {'id': 945554174, 'node_id': 'R_kgDOOFwC_g', 'name': 'AI_Assistant', 'full_name': 'appsbotta/AI_Assistant', 'private': False, 'owner': {'login': 'appsbotta', 'id': 75985363, 'node_id': 'MDQ6VXNlcjc1OTg1MzYz', 'avatar_url': 'https://avatars.githubusercontent.com/u/75985363?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/appsbotta', 'html_url': 'https://github.com/appsbotta', 'followers_url': 'https://api.github.com/users/appsbotta/followers', 'following_url': 'https://api.github.com/users/appsbotta/following{/other_user}', 'gists_url': 'https://api.github.com/users/appsbotta/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/appsbotta/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/appsbotta/subscriptions', 'organizations_url': 'https://api.github.com/users/appsbotta/orgs', 'repos_url': 'https://api.github.com/users/appsbotta/repos', 'events_url': 'https://api.github.com/users/appsbotta/events{/privacy}', 'received_events_url': 'https://api.github.com/users/appsbotta/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False}, 'html_url': 'https://github.com/appsbotta/AI_Assistant', 'description': None, 'fork': False, 'url': 'https://api.github.com/repos/appsbotta/AI_Assistant', 'forks_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/forks', 'keys_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/keys{/key_id}', 'collaborators_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/collaborators{/collaborator}', 'teams_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/teams', 'hooks_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/hooks', 'issue_events_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues/events{/number}', 'events_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/events', 'assignees_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/assignees{/user}', 'branches_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/branches{/branch}', 'tags_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/tags', 'blobs_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/blobs{/sha}', 'git_tags_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/tags{/sha}', 'git_refs_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/refs{/sha}', 'trees_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/trees{/sha}', 'statuses_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/statuses/{sha}', 'languages_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/languages', 'stargazers_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/stargazers', 'contributors_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/contributors', 'subscribers_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/subscribers', 'subscription_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/subscription', 'commits_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/commits{/sha}', 'git_commits_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/commits{/sha}', 'comments_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/comments{/number}', 'issue_comment_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues/comments{/number}', 'contents_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/contents/{+path}', 'compare_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/compare/{base}...{head}', 'merges_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/merges', 'archive_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/{archive_format}{/ref}', 'downloads_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/downloads', 'issues_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues{/number}', 'pulls_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/pulls{/number}', 'milestones_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/milestones{/number}', 'notifications_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/notifications{?since,all,participating}', 'labels_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/labels{/name}', 'releases_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/releases{/id}', 'deployments_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/deployments', 'created_at': '2025-03-09T17:38:25Z', 'updated_at': '2025-03-16T16:46:50Z', 'pushed_at': '2025-03-16T16:46:47Z', 'git_url': 'git://github.com/appsbotta/AI_Assistant.git', 'ssh_url': 'git@github.com:appsbotta/AI_Assistant.git', 'clone_url': 'https://github.com/appsbotta/AI_Assistant.git', 'svn_url': 'https://github.com/appsbotta/AI_Assistant', 'homepage': None, 'size': 63, 'stargazers_count': 0, 'watchers_count': 0, 'language': 'Jupyter Notebook', 'has_issues': True, 'has_projects': True, 'has_downloads': True, 'has_wiki': True, 'has_pages': False, 'has_discussions': False, 'forks_count': 0, 'mirror_url': None, 'archived': False, 'disabled': False, 'open_issues_count': 0, 'license': {'key': 'mit', 'name': 'MIT License', 'spdx_id': 'MIT', 'url': 'https://api.github.com/licenses/mit', 'node_id': 'MDc6TGljZW5zZTEz'}, 'allow_forking': True, 'is_template': False, 'web_commit_signoff_required': False, 'topics': [], 'visibility': 'public', 'forks': 0, 'open_issues': 0, 'watchers': 0, 'default_branch': 'main', 'permissions': {'admin': True, 'maintain': True, 'push': True, 'triage': True, 'pull': True}}]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"artifacts/data_ingestion\",'data.json'),'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "data\n",
    "for i,repi in enumerate(data):\n",
    "        print(type(repi))\n",
    "        break\n",
    "bot = data[:2]\n",
    "print(bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self,config:DataTrasformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_data(self):\n",
    "        file_dir = self.config.file_dir\n",
    "        for file_name in os.listdir(file_dir):\n",
    "            if file_name.lower().endswith(\".pdf\"):  # Check if file is a PDF\n",
    "                source_path = os.path.join(file_dir, file_name)\n",
    "                destination_path = os.path.join(self.config.save_dir, file_name)\n",
    "                shutil.copy2(source_path, destination_path)\n",
    "        \n",
    "        with open(os.path.join(file_dir,'data.json'),'r',encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(type(data))\n",
    "        print(type(data[0]))\n",
    "        return data\n",
    "    \n",
    "    def get_lang(self,repo_name):\n",
    "        url = f'https://api.github.com/repos/appsbotta/{repo_name}/languages'\n",
    "        headers = {\"Authorization\": f\"token {os.getenv('TOKEN')}\"}\n",
    "        lang = requests.get(url,headers=headers).json()\n",
    "        top_three_keys = sorted(lang, key=lang.get, reverse=True)[:3]\n",
    "        # logger.info(f\"Got top 3 languages used in {repo_name}\")\n",
    "        return top_three_keys\n",
    "    \n",
    "    def get_timings(self,repo):\n",
    "        update = repo['updated_at'][:10]\n",
    "        created = repo['created_at'][:10]\n",
    "        pushed = repo['pushed_at'][:10]\n",
    "        times = {\n",
    "            \"updated_at\": update,\n",
    "            \"created_at\":created,\n",
    "            \"pushed_at\": pushed,\n",
    "        }\n",
    "        # logger.info(f\"secured 3 timings {times.keys()}\")\n",
    "        return times\n",
    "    \n",
    "    def get_readme(self,OWNER,REPO):\n",
    "        # logger.info(f\"requeting readme file for the repo {REPO} \")\n",
    "        readme = f\"https://api.github.com/repos/{OWNER}/{REPO}/readme\"\n",
    "        headers = {\"Authorization\": f\"token {os.getenv('TOKEN')}\"}\n",
    "        response = requests.get(readme,headers=headers)\n",
    "        readme_content = \"No Readme FIle\"\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            readme_content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
    "            readme_content =  markdown2.markdown(readme_content, extras=[\"strip\"])\n",
    "            readme_content = html2text.html2text(readme_content)\n",
    "        # logger.info(f\"Request for README file for the repo {REPO} completed\")\n",
    "        return readme_content\n",
    "            \n",
    "    def get_pdf_from_data(self):\n",
    "        logger.info(\"Saving data to a pdf file started\")\n",
    "        save_dir = self.config.save_dir\n",
    "        data = self.get_data()\n",
    "\n",
    "        doc = SimpleDocTemplate(os.path.join(save_dir,'repo.pdf'), pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        custom_style = ParagraphStyle(\n",
    "            'Custom',\n",
    "            parent=styles['Normal'],\n",
    "            fontName='Courier',\n",
    "            fontSize=10,\n",
    "            leading=14,  # Line height\n",
    "            spaceAfter=5,\n",
    "        )\n",
    "        heading_style = ParagraphStyle(\n",
    "            \"HeadingStyle\",\n",
    "            parent=styles[\"Heading1\"],\n",
    "            fontName=\"Courier-Bold\",\n",
    "            fontSize=14,\n",
    "            leading=22,\n",
    "\n",
    "        )\n",
    "\n",
    "        content = []\n",
    "\n",
    "\n",
    "\n",
    "        for i,repo in enumerate(data):\n",
    "            text = f\"{i+1}. <b>{repo['name']}</b>  --->   {repo['html_url']} \\n\"\n",
    "            # text = text + str(i+1) + \". \" +  str(repo[\"name\"]) + \" -> \" + str(repo[\"html_url\"]) +\"\\n\"\n",
    "            paragraph = Paragraph(text,custom_style)\n",
    "            content.append(paragraph)\n",
    "        content.append(Spacer(1,5))\n",
    "        \n",
    "        for i,repo in enumerate(data):\n",
    "            OWNER = repo['owner']['login']\n",
    "            REPO = repo['name']\n",
    "            readme = self.get_readme(OWNER,REPO)\n",
    "            timings = self.get_timings(repo)\n",
    "            lang = self.get_lang(REPO)\n",
    "            heading = Paragraph(REPO,heading_style)\n",
    "            content.append(heading)\n",
    "            for key,value in timings.items():\n",
    "                text = f\"{key} -> {value}\"\n",
    "                paragraph = Paragraph(text,custom_style)\n",
    "                content.append(paragraph)\n",
    "            lang_text = \"   \"\n",
    "            for it in lang:\n",
    "                lang_text = lang_text + it +\", \"\n",
    "            text = Paragraph(lang_text,custom_style)\n",
    "            content.append(text)\n",
    "            readme = Preformatted(readme,custom_style)\n",
    "            content.append(readme)\n",
    "            content.append(Spacer(1, 5))\n",
    "\n",
    "\n",
    "        doc.build(content)\n",
    "        logger.info(\"saving data to a pdf file is completed\")\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-16 22:07:46,279: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-16 22:07:46,287: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-16 22:07:46,290: INFO: common: created directory at: artifacts]\n",
      "[2025-03-16 22:07:46,293: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-03-16 22:07:46,296: INFO: 158897180: Saving data to a pdf file started]\n",
      "[2025-03-16 22:08:09,016: INFO: 158897180: saving data to a pdf file is completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.get_pdf_from_data()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# DL-End _to_ End\\n\\n## Workflow\\n\\n  1. Update config.yaml in config dir\\n  2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity\\n  5. Update the configuration manager in src config\\n  6. Update the components\\n  7. Update the pipeline\\n  8. Update the main.py\\n  9. Update the dvc.yaml\\n\\n# To run the repo\\n\\n## steps\\n\\nClone the repository `bash https://github.com/appsbotta/DL-End_to_End `\\n\\n## step - 01 Create a conda env after opening the repo\\n\\n`bash conda create -n ckn python=3.13 -y ` `bash conda activate ckn `\\n\\n## step - 02 Install Requirements.txt\\n\\n`bash pip install -r requirements.txt `\\n\\n## step - 03 Run app.py\\n\\n`bash python app.py `\\n\\n## DVC cmd\\n\\n  * dvc init\\n  * dvc repro\\n  * dvc dag\\n\\n# AWS CI-CD Deployment using Github Actions\\n\\n## 1\\\\. Login to AWS Console\\n\\n## 2\\\\. Create IAM user for deployment\\n\\n```bash\\n\\n# give these access to IAM user\\n\\n  1. EC2 access -> this is virtual machine\\n  2. ECR: Elastic COntainer registery to save your docker image in aws\\n\\n# About deployment\\n\\n  1. build docker image of the source code\\n\\n  2. Push your docker image to ECR\\n\\n  3. Launch your EC2 instance\\n\\n  4. Pull your image from ECR to EC2\\n\\n  5. Launch your docker image in EC2\\n\\n# policy\\n\\n  1. AmazonEC2ContainerRegistryFullAccess\\n\\n  2. AmazonEC2FullAccess\\n\\n```\\n\\n## 3\\\\. Create ECR repo to store /dave docker image\\n\\n`bash copy the repo uri ex : 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com/chicken `\\n\\n## 4\\\\. Create an EC2 machine Instance(Ubuntu)\\n\\n## 5\\\\. Connect EC2 instance and install docker\\n\\n```bash\\n\\n# optinal\\n\\nsudo apt-get update -y\\n\\nsudo apt-get upgrade\\n\\n# required\\n\\ncurl -fsSL https://get.docker.com -o get-docker.sh\\n\\nsudo sh get-docker.sh\\n\\nsudo usermod -aG docker ubuntu\\n\\nnewgrp docker ```\\n\\n## 6\\\\. create a self runner in github repo settings\\n\\n`bash in settings under action/runner create a new runner. a. while running\\nthe cmds in order when asked for runner group keep it default but for runner\\nname use self-hosted `\\n\\n## 7\\\\. Setup github secrets\\n\\n```bash AWS _ACCESS_ KEY_ID=\\n\\nAWS _SECRET_ ACCESS_KEY=\\n\\nAWS_REGION = us-east-1\\n\\nAWS _ECR_ LOGIN_URI = ''(for example it will be 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com)\\n\\nECR _REPOSITORY_ NAME = '' (example chicken as last part of ECR uri)\\n\\n\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_readme(OWNER,REPO):\n",
    "        readme = f\"https://api.github.com/repos/{OWNER}/{REPO}/readme\"\n",
    "        headers = {\"Authorization\": f\"token {os.getenv(\"TOKEN\")}\"}\n",
    "        response = requests.get(readme,headers=headers)\n",
    "        readme_content = \"No Readme FIle\"\n",
    "        print(response.status_code)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            readme_content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
    "            readme_content =  markdown2.markdown(readme_content, extras=[\"strip\"])\n",
    "            readme_content = html2text.html2text(readme_content)\n",
    "        return readme_content\n",
    "\n",
    "content = get_readme('appsbotta','DL-End_to_End')\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # with open(os.path.join(file_path,'repo.txt'), \"r\", encoding=\"utf-8\") as f:\n",
    "        #     lines = f.readlines()\n",
    "        #     for line in lines:\n",
    "        #         if line.strip() == \"<=================================================================================================>\":\n",
    "        #             paragraph = Preformatted(line.strip(), custom_style)\n",
    "        #         else:\n",
    "        #             paragraph = Paragraph(line.strip(), custom_style)     # Wrap text automatically\n",
    "        #         content.append(paragraph)\n",
    "        #         content.append(Spacer(1, 5))  # Preserve spacing\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import markdown2\n",
    "import html2text\n",
    "from bot import logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from bot.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self,config:DataIngestionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def get_lang(self,repo_name):\n",
    "        url = f'https://api.github.com/repos/appsbotta/{repo_name}/languages'\n",
    "        lang = requests.get(url).json()\n",
    "        top_three_keys = sorted(lang, key=lang.get, reverse=True)[:3]\n",
    "        return top_three_keys\n",
    "    \n",
    "    def get_timings(self,repo):\n",
    "        update = repo['updated_at'][:10]\n",
    "        created = repo['created_at'][:10]\n",
    "        pushed = repo['pushed_at'][:10]\n",
    "        return [update,created,pushed]\n",
    "    \n",
    "    def get_readme(self,OWNER,REPO):\n",
    "        readme = f\"https://api.github.com/repos/{OWNER}/{REPO}/readme\"\n",
    "        response = requests.get(readme)\n",
    "        readme_content = \"No Readme FIle\"\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            readme_content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
    "            readme_content =  markdown2.markdown(readme_content, extras=[\"strip\"])\n",
    "            readme_content = html2text.html2text(readme_content)\n",
    "        return readme_content\n",
    "\n",
    "    def get_repos(self):\n",
    "        url = self.config.github_url\n",
    "        headers = {\"Authorization\": f\"token {os.getenv(\"TOKEN\")}\"}\n",
    "        repos = requests.get(url, headers=headers).json()\n",
    "        save_dir = self.config.save_dir\n",
    "        with open(os.path.join(save_dir,'repo.txt'),'w',encoding='utf-8') as f:\n",
    "            f.write(str(\"\\n\"))\n",
    "            for i,repo in enumerate(repos):\n",
    "                OWNER = repo['owner']['login']\n",
    "                REPO = repo['name']\n",
    "                timings = self.get_timings(REPO)\n",
    "                readme_content = self.get_readme(OWNER,REPO)\n",
    "                f.write( str(i+1) + \". \" +  str(repo[\"name\"])+\"\\n\")\n",
    "                f.write(\"Description \\n\")\n",
    "                f.write(readme_content)\n",
    "                f.write(str(\"\\n\"))\n",
    "                f.write(str(\"\\n\"))\n",
    "                f.write(\"<=================================================================================================>\")\n",
    "                f.write(str(\"\\n\"))\n",
    "                f.write(str(\"\\n\"))\n",
    "            logger.info(f\"Completed writing data from git hub repos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-11T05:05:33+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'artifacts\\\\data_transformation\\\\Profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"Contact\\n9842345676 (Mobile)\\nappsbotta@gmail.com\\nwww.linkedin.com/in/lokesh5489\\n(LinkedIn)\\nTop Skills\\nAgentic AI\\nGoogle Gemini\\nGenerative AI\\nLokesh Apparao Botta\\nData Science | Agentic AI | NLP | IIT Guwahati'24\\nNarsipatnam, Andhra Pradesh, India\\nSummary\\nI am graduate in electronics and communications engineering at IIT\\nGuwahati, one of the premier institutes of technology in India. I have\\na strong academic background and Deep Learning and Machine\\nLearning skills. My areas of interest are machine learning, deep\\nneural networks and computer networks. I aspire to pursue a career\\nin these domains, and contribute to the advancement of science\\nand technology. Along with this i have a working experience as a\\nresearch intern on resource allocation for D2D communication.\\nExperience\\nIndian Institute of Technology, Guwahati\\nResearch Intern\\nMay 2023\\xa0-\\xa0March 2024\\xa0(11 months)\\nGuwahati, Assam, India\\n-Engaged in research and projects alongside Professor Sukumar Nandi.\\n-Developing a graph based algorithm for resource allocation for Device-to-\\nDevice communication for better Quality of Service for both cellular and D2D\\nusers.\\nEducation\\nIndian Institute of Technology, Guwahati\\nBachelor of Technology - B.Tech,\\xa0Electrical, Electronics and Communications\\nEngineering\\xa0·\\xa0(November 2020\\xa0-\\xa0July 2024)\\nSri Chaitanya College of Education\\nIntermediate\\xa0·\\xa0(2018\\xa0-\\xa02020)\\nSri Chaitanya College of Education\\n10th,SSC\\xa0·\\xa0(July 2017\\xa0-\\xa0March 2018)\\n\\xa0 Page 1 of 1\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='1. AgenticAi ---> https://github.com/appsbotta/AgenticAi\\n2. AI_Assistant ---> https://github.com/appsbotta/AI_Assistant\\n3. AWS_BEDROCK ---> https://github.com/appsbotta/AWS_BEDROCK\\n4. DataSets ---> https://github.com/appsbotta/DataSets\\n5. DL-End_to_End ---> https://github.com/appsbotta/DL-End_to_End\\n6. FaceDetection_React ---> https://github.com/appsbotta/FaceDetection_React\\n7. FitnessTracker ---> https://github.com/appsbotta/FitnessTracker\\n8. Gemini ---> https://github.com/appsbotta/Gemini\\n9. ImageCaption ---> https://github.com/appsbotta/ImageCaption\\n10. JosaaDataAnalysis ---> https://github.com/appsbotta/JosaaDataAnalysis\\n11. Kidney_diease_end_to_end --->\\nhttps://github.com/appsbotta/Kidney_diease_end_to_end\\n12. LangChain ---> https://github.com/appsbotta/LangChain\\n13. Llamaindex ---> https://github.com/appsbotta/Llamaindex\\n14. Market_Basket ---> https://github.com/appsbotta/Market_Basket\\n15. Networks ---> https://github.com/appsbotta/Networks\\n16. OlX_CLONE ---> https://github.com/appsbotta/OlX_CLONE\\n17. Portifolio ---> https://github.com/appsbotta/Portifolio\\n18. Q-A_ChatBot_with_Implementation --->\\nhttps://github.com/appsbotta/Q-A_ChatBot_with_Implementation\\n19. Real-Time-chatting ---> https://github.com/appsbotta/Real-Time-chatting\\n20. SentimentAnalysis ---> https://github.com/appsbotta/SentimentAnalysis\\n21. SimpleDVC ---> https://github.com/appsbotta/SimpleDVC\\n22. StockPrediction ---> https://github.com/appsbotta/StockPrediction\\n23. student_performance --->\\nhttps://github.com/appsbotta/student_performance\\n24. Telecom_Churn_Prediction --->\\nhttps://github.com/appsbotta/Telecom_Churn_Prediction\\n25. Text-Summarizer ---> https://github.com/appsbotta/Text-Summarizer\\n26. Video_Web ---> https://github.com/appsbotta/Video_Web\\n1. AgenticAi\\nupdated_at -> 2025-03-10\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-10'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Python,\\nNo Readme FIle\\n2. AI_Assistant\\nupdated_at -> 2025-03-10\\ncreated_at -> 2025-03-09\\npushed_at -> 2025-03-10\\nJupyter Notebook, Python,\\n# AI_Assistant\\n3. AWS_BEDROCK\\nupdated_at -> 2025-03-07\\ncreated_at -> 2025-03-06\\npushed_at -> 2025-03-07\\nPython,\\n# AWS_BEDROCK\\n# to run this\\n## 1\\\\. Git clone the repo\\n## 2\\\\. Install requirements.txt\\n## 3\\\\. Create an IAM user with Administrtor access\\n## 4\\\\. Configure AWS in cli\\n`bash aws configure `\\n4. DataSets\\nupdated_at -> 2025-03-03\\ncreated_at -> 2023-08-27\\npushed_at -> 2025-03-03\\nJupyter Notebook,\\nNo Readme FIle\\n5. DL-End_to_End'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='updated_at -> 2025-03-05\\ncreated_at -> 2025-03-02\\npushed_at -> 2025-03-05\\nJupyter Notebook, Python, HTML,\\n# DL-End _to_ End\\n## Workflow\\n  1. Update config.yaml in config dir\\n  2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity\\n  5. Update the configuration manager in src config\\n  6. Update the components\\n  7. Update the pipeline\\n  8. Update the main.py\\n  9. Update the dvc.yaml\\n# To run the repo\\n## steps\\nClone the repository `bash https://github.com/appsbotta/DL-End_to_End `\\n## step - 01 Create a conda env after opening the repo\\n`bash conda create -n ckn python=3.13 -y ` `bash conda activate ckn `\\n## step - 02 Install Requirements.txt\\n`bash pip install -r requirements.txt `\\n## step - 03 Run app.py\\n`bash python app.py `\\n## DVC cmd\\n  * dvc init\\n  * dvc repro\\n  * dvc dag\\n# AWS CI-CD Deployment using Github Actions'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='## 1\\\\. Login to AWS Console\\n## 2\\\\. Create IAM user for deployment\\n```bash\\n# give these access to IAM user\\n  1. EC2 access -> this is virtual machine\\n  2. ECR: Elastic COntainer registery to save your docker image in aws\\n# About deployment\\n  1. build docker image of the source code\\n  2. Push your docker image to ECR\\n  3. Launch your EC2 instance\\n  4. Pull your image from ECR to EC2\\n  5. Launch your docker image in EC2\\n# policy\\n  1. AmazonEC2ContainerRegistryFullAccess\\n  2. AmazonEC2FullAccess\\n```\\n## 3\\\\. Create ECR repo to store /dave docker image\\n`bash copy the repo uri ex : 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com/chicken `\\n## 4\\\\. Create an EC2 machine Instance(Ubuntu)\\n## 5\\\\. Connect EC2 instance and install docker\\n```bash\\n# optinal\\nsudo apt-get update -y'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content=\"sudo apt-get upgrade\\n# required\\ncurl -fsSL https://get.docker.com -o get-docker.sh\\nsudo sh get-docker.sh\\nsudo usermod -aG docker ubuntu\\nnewgrp docker ```\\n## 6\\\\. create a self runner in github repo settings\\n`bash in settings under action/runner create a new runner. a. while running\\nthe cmds in order when asked for runner group keep it default but for runner\\nname use self-hosted `\\n## 7\\\\. Setup github secrets\\n```bash AWS _ACCESS_ KEY_ID=\\nAWS _SECRET_ ACCESS_KEY=\\nAWS_REGION = us-east-1\\nAWS _ECR_ LOGIN_URI = ''(for example it will be 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com)\\nECR _REPOSITORY_ NAME = '' (example chicken as last part of ECR uri)\\n6. FaceDetection_React\\nupdated_at -> 2023-08-19\\ncreated_at -> 2023-08-19\\npushed_at -> 2023-08-19\\nJavaScript, HTML, CSS,\\n# facedetection\\n7. FitnessTracker\\nupdated_at -> 2023-08-27\\ncreated_at -> 2023-08-27\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='pushed_at -> 2023-08-27\\nJavaScript, HTML, CSS,\\n# Fitness Tracker\\n## Description\\nThis is a Fitness Tracker built using MERN stack. It facilitates one to add\\nusers and record their date and duration of performing particular fitness\\nactivity.Activity can also be deleted. This web application is built using\\nMongoDB,Express,Nodejs,React and Material UI. Images used in this application\\nare being taken from [Unsplash website](https://unsplash.com).\\n## Demo\\n■ [Link](https://serene-volhard-1843cb.netlify.app/)\\nBackend Heroku Link-https://fitness-tracker-mern.herokuapp.com/\\n[Users](https://fitness-tracker-mern.herokuapp.com/users)\\n[Exercises](https://fitness-tracker-mern.herokuapp.com/exercises)\\n## Screenshots\\nHomepage ![fitkit_home](https://user-\\nimages.githubusercontent.com/4997491/117522296-20021d80-afd0-11eb-8079-922a4d51925a.JPG)\\nCreate user ![fitkit_user](https://user-\\nimages.githubusercontent.com/4997491/117525054-3911cb00-afde-11eb-97b5-a468f17935de.JPG)\\nRecord Fitness Activity ![fitkit_exercise](https://user-\\nimages.githubusercontent.com/4997491/117525061-3d3de880-afde-11eb-9201-acdb5b804f40.JPG)\\nFitness Activity Dashboard ![fitkit_dashboard](https://user-\\nimages.githubusercontent.com/4997491/117525087-652d4c00-afde-11eb-92d4-0fda6f5c5d03.JPG)\\n404 Error page ![error](https://user-\\nimages.githubusercontent.com/4997491/117525227-dcfb7680-afde-11eb-9434-a8f93c5b76d7.JPG)\\n## Available Scripts\\nIn the project directory, you can run:\\n`npm start`'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Runs the app in the development mode.  \\nOpen <http://localhost:3000> to view it in the browser.\\n8. Gemini\\nupdated_at -> 2025-02-16\\ncreated_at -> 2025-01-16\\npushed_at -> 2025-02-16\\nPython,\\nNo Readme FIle\\n9. ImageCaption\\nupdated_at -> 2024-08-04\\ncreated_at -> 2023-06-20\\npushed_at -> 2024-08-04\\nPython, HTML,\\n# image-caption-generator\\n## Deployed Link\\n[Image Caption](https://image-caption-gen-deploy-1.streamlit.app/)\\n# Home page\\n![Screenshot \\\\(82\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/33837fd6-2e5a-4329-9038-5034fecc4e11)\\n# Upon selecting caption predictor\\n![Screenshot \\\\(470\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/20c43357-01e9-4c5d-94ad-6182d370a625)\\n# Sample Images\\n![Screenshot \\\\(28\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/f6fe65e4-964e-48f0-98ef-703cdd872750) ![Screenshot\\n\\\\(27\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/492b7abd-e277-4e06-b2d0-a8ffef509597)\\n10. JosaaDataAnalysis\\nupdated_at -> 2023-08-28'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content=\"created_at -> 2023-08-28\\npushed_at -> 2023-08-28\\nHTML, Python, CSS,\\n# JOSAA-data-analysis\\nThis project aims to create a portal that allows users to explore the seat\\nallotment statistics of JOSAA (Joint Seat Allocation Authority) from 2016 to\\n2022. The project involves web scraping data from the JOSAA website,\\nperforming data cleaning and analysis, and presenting the insights through\\nvisualizations on a website frontend. The website presents the findings\\nthrough interactive charts and tables, enabling users to gain valuable\\ninsights into the trends and patterns of seat allotments over the years.\\n## Tech Stack/Frameworks:\\n  * Frontend: HTML, CSS, JavaScript\\n  * Backend: Django\\n  * Database: SQLite\\n  * Data Scraping: Beautiful Soup\\n  * Data Cleaning: NumPy, Pandas\\n  * Data Visualization: Chart.js\\n## Command to run the project locally\\n  * python manage.py runserver\\n  * Then run the server locally on port 8000\\n## Home page\\nThe home page of the JOSAA Analysis portal provides a user-friendly interface\\nto explore and analyze the JOSAA seat allotment statistics. It offers several\\ninsightful sections to delve into the data and gain valuable information.\\nHere's a breakdown of each section: \\\\- View Institute wise cut-off \\\\- Analyze\\ninstitute wise cut-off trends \\\\- Roundwise cut-off Analysis\\n  * ![home](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/1d752e47-ef2d-4b4c-bf66-77fc8817c09d)\\n## View Institute wise cut-off list page\\n  * Upon clicking this option, a list of all IITs is shown along with their NIRF ranking(2023), Location and their established year. So the users can select specific institutes and explore the corresponding cut-off data.\\n  * ![iit list](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/f4f562e1-c93b-48a5-a114-4c321b296fe6)\\n    * ### For each IIT \\n      * By selecting a specific IIT, users can explore the trends and changes in the cut-off ranks throughout the various rounds of the seat allocation process over the years 2016 to 2022.\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='* It can further narrow down their search by choosing specific programs offered by the selected institutes along with seat type and gender.\\n      * This feature helps users understand the cut-off trends for different programs in their preferred institutes.\\n      * ![first graph](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/5c411f78-ea17-4218-a0d6-9a8740ad78b5)\\n## Analyze institute wise cut-off trends\\n  * Institute trends highlight the trends of various programs offered by a particular institute over the years. This helps understand the popularity and perception of programs offered by the institute, and thus helps understand the demand for a particular program in the institute during the counselling process.\\n  * ![Screenshot \\\\(20\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/45d3feab-6bf4-4611-91fb-91580b6a90ce)\\n  * Upon sleecting the gender, branch and institute, user will get the cut-off analysis for each branch present at that IIT.\\n  * ![Screenshot \\\\(22\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/1db046fe-c44c-4627-8282-c8143bf0a88a)\\n## Roundwise cut-off Analysis\\n  * Round trends highlight the general trend of closing ranks throughout the rounds of the counselling process. This helps understand the likely range of changes to the closing ranks throught the counselling proces.\\n  * Users can get the round wise analysis at each IIT by selecting the gender,seat type and branch. \\n  * User can change the college name by clicking on \"Go to iit-list\" buttion.\\n  * ![Screenshot \\\\(25\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/3206dd19-d8fb-4e88-ace9-84a743db723e)\\n11. Kidney_diease_end_to_end\\nupdated_at -> 2025-03-05\\ncreated_at -> 2025-03-05\\npushed_at -> 2025-03-05\\nPython, Jupyter Notebook,\\n# Kidney _diease_ end _to_ end\\n## Workflow\\n  1. Update config.yaml in config dir\\n  2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity\\n  5. Update the configuration manager in src config\\n  6. Update the components\\n  7. Update the pipeline\\n  8. Update the main.py\\n  9. Update the dvc.yaml\\n# To run the repo\\n## steps'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Clone the repository `bash\\nhttps://github.com/appsbotta/Kidney_diease_end_to_end `\\n## step - 01 Create a conda env after opening the repo\\n`bash conda create -n ckn python=3.8 -y ` `bash conda activate ckn `\\n## step - 02 Install Requirements.txt\\n`bash pip install -r requirements.txt `\\n## step - 03 Run app.py\\n`bash python app.py `\\n## DVC cmd\\n  * dvc init\\n  * dvc repro\\n  * dvc dag\\n# AWS CI-CD Deployment using Github Actions\\n## 1\\\\. Login to AWS Console\\n## 2\\\\. Create IAM user for deployment\\n```bash\\n# give these access to IAM user\\n  1. EC2 access -> this is virtual machine\\n  2. ECR: Elastic COntainer registery to save your docker image in aws\\n# About deployment\\n  1. build docker image of the source code\\n  2. Push your docker image to ECR\\n  3. Launch your EC2 instance\\n  4. Pull your image from ECR to EC2\\n  5. Launch your docker image in EC2'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='# policy\\n  1. AmazonEC2ContainerRegistryFullAccess\\n  2. AmazonEC2FullAccess\\n```\\n12. LangChain\\nupdated_at -> 2025-03-09\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-09\\nJupyter Notebook, Python,\\n# LangChain Projects\\nThis repository showcases various applications and experiments utilizing the\\nLangChain framework, which is designed for building applications powered by\\nlarge language models (LLMs).\\n## Table of Contents\\n  * Projects\\n    * LLMAPP\\n    * PdfQuery\\n    * ChatMultipleDocs\\n    * TextSummarization\\n    * Celebrity.py\\n  * Installation\\n  * Usage\\n  * Contributing\\n  * License\\n## Projects\\n### LLMAPP\\nA project demonstrating the integration of LangChain with large language\\nmodels to build intelligent applications.\\n### PdfQuery\\nAn application that utilizes LangChain to extract and process information from\\nPDF documents.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='### ChatMultipleDocs\\nA chatbot implementation capable of interacting with multiple documents,\\npowered by LangChain.\\n### TextSummarization\\nA tool that leverages LangChain to provide text summarization capabilities.\\n### Celebrity.py\\nA script that explores web to get the data related to a Celebrity given his\\nname and any 5 major events happend around his DOB\\n## Installation\\n  1. **Clone the repository:**\\n```bash git clone https://github.com/appsbotta/LangChain.git cd LangChain\\n  2. **Create a virtual environment:**\\n```bash conda create -p myenv python conda activate path/to/myenv\\n  3. **Install the required dependencies:**\\n```bash pip install -r requirements.txt\\n  4. **Create a`.env` file in the root of the repository:**\\nAdd your API keys to the .env file ```bash touch .env\\n## Usage\\nEach project is contained within its respective directory. Navigate to the\\ndesired project folder and follow the instructions provided above\\n13. Llamaindex\\nupdated_at -> 2025-01-17\\ncreated_at -> 2025-01-17\\npushed_at -> 2025-01-17\\nJupyter Notebook,\\n# Llamaindex'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='14. Market_Basket\\nupdated_at -> 2023-08-15\\ncreated_at -> 2023-08-15\\npushed_at -> 2023-08-15\\nJupyter Notebook,\\n# Market_Basket\\nThis notebook has been executed on kaggle. To view the notebook and run it,\\nhttps://www.kaggle.com/code/lokesh5489/market-basket To view the concerned\\ndataset, visit https://www.kaggle.com/code/lokesh5489/market-basket/data\\nUsed Aprior algorithm to find relation between two products\\n15. Networks\\nupdated_at -> 2024-05-25\\ncreated_at -> 2024-05-23\\npushed_at -> 2024-05-25\\nC++, HTML, C,\\n# # The Network Simulator, Version 3\\n## Changes that we made\\n### 1.lte-ue-mac.cc\\n  * added one pre-defined table for type-0 allocation at line 52\\n  * Added Pf in line 404\\n  * Added function to get rbg size at line 503\\n  * line 2482 for grant printing in random allocation\\n  * line 2532 code for PF allocation, CQI calculation,mcs from cqi\\n  * added function at line 2715\\n### 2.lte-sl-out-of-covrg-comm.cc\\n  * At line 268 added code to print from rrc\\n  * At line 412 added code to print from mac\\n16. OlX_CLONE\\nupdated_at -> 2023-08-26\\ncreated_at -> 2023-08-26'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='pushed_at -> 2023-08-26\\nJavaScript, CSS, HTML,\\n# OLX _clone_ IITG\\n## About the website\\nUsers are directed to login page on opening the website. Users have to login\\nusing their Outlook Id. Later user will be directed to home page of the\\nwebsite where two buttons will be displayed. Here user can choose whether to\\nbuy or sell and based on that available categories will be diplayed. Profile\\nshows logged in users profile. Logout button logs out the user.\\n## Environment Setup Locally\\n  * Install React\\n  * Use npm install in both Backend and Client \\n  * ...\\n## Tech Stack Used\\n  * React\\n  * MongoDB\\n  * Express\\n  * Node\\n  * HTML\\n  * CSS\\n  * Javascript\\n## Key Features\\n  * User can login and logout using Outlook Id\\n  * Functionality to buy and sell\\n  * Seller Profile Containing all the products he want to sell and stats of every deal he made, like people who are interested with listed money\\n  * Search functionality for buyer and filter method to get his wanted product within the desired price range\\n  * Product details along with buyer details\\n  * Option to agree to listed deal or to reach out seller for some negotiation for a particular product\\n  * Marks product as sold if deal is made\\n## Login page\\n  * The web application is authenticated and user must sign in using Outlook Id. ![image](https://user-images.githubusercontent.com/95306028/177820740-33b49604-b1b1-4285-bc41-9b9a5bf5b1ce.png)\\n## Home page\\n  * After login the home page will be displayed, where user can choose to buy or sell items. ![image](https://user-images.githubusercontent.com/95306028/177821314-90828c2c-6ef6-4644-9729-df123930b31c.png)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content=\"## Buyer Homepage\\n  * After clicking **Buy** all categories which are available are displayed. User can also search for required item in search bar.\\n  * ![image](https://user-images.githubusercontent.com/95306028/177822651-b505f91e-2f11-4826-951c-e6e6e0560d97.png)\\n17. Portifolio\\nupdated_at -> 2025-03-04\\ncreated_at -> 2025-01-18\\npushed_at -> 2025-03-04\\nJavaScript, HTML, CSS,\\n# Portifolio\\n18. Q-A_ChatBot_with_Implementation\\nupdated_at -> 2025-01-08\\ncreated_at -> 2025-01-07\\npushed_at -> 2025-01-08\\nPython,\\n* * *\\ntitle: LangChain ChatBot emoji: ■ colorFrom: gray colorTo: blue sdk: streamlit\\nsdk _version: 1.41.1 app_ file: app.py\\n## pinned: false\\n19. Real-Time-chatting\\nupdated_at -> 2022-06-30\\ncreated_at -> 2022-06-30\\npushed_at -> 2022-07-07\\nHTML, CSS, JavaScript,\\n# Real-Time-chatting\\nUser are directed to opening page where ther need to click 'Enter ChatRoom',\\nUpon Clicking it users are redirected to a page where they need to enter then\\nroom,upon that the user will enter into chat room where he can chat with his\\nother buddies!!\\n## Enviroment Setup Locally\\n  * Install Web Sockets and Expressjs\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='* Run the server and Connect to Port 500\\n## Tech Stack Used\\n  * HTML\\n  * JavaScript\\n  * Css\\n  * WebSockets\\n## Opening Page\\n  * This the Page we get when we Open localhost in port 5000\\n![OPENING](https://user-\\nimages.githubusercontent.com/75985363/177767489-6a32f337-8b3c-4281-a281-1d58c24ae3e0.jpg)\\n## Name Entry\\n  * User need to enter his name ![entering name](https://user-images.githubusercontent.com/75985363/177767789-5137808d-a34a-4bed-b415-b3ab67a61fca.jpg)\\n## Chat Room\\n  * After entering name User redirects to this page ![chat room](https://user-images.githubusercontent.com/75985363/177767981-0893a5eb-7aa2-4421-8bd1-da9592eeb46d.jpg)\\n20. SentimentAnalysis\\nupdated_at -> 2024-11-09\\ncreated_at -> 2024-11-09\\npushed_at -> 2024-11-09\\n# SentimentAnalysis\\n21. SimpleDVC\\nupdated_at -> 2025-02-24\\ncreated_at -> 2025-02-21\\npushed_at -> 2025-02-24\\nPython, Jupyter Notebook, HTML,\\n# WorkFlow of the Project\\n  1. Create env `bash conda create -p myenv python -y `\\n  2. activate env `bash conda activate (path to myenv) `'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='3. Create Requirements.txt\\n  4. Install requirements.txt `bash pip install -r requirements.txt `\\n  5. Initilize git `bash git init `\\n  6. Initilize DVC `bash dvc init `\\n  7. Add data to DVC `bash dvc add data_given/winequality.csv `\\n  8. `bash git add . && git commit -m \"first commit\" git push origin main `\\n  9. Create params.yaml\\n  10. create get _data.py and load_ data.py to get data and add the load_data stage to dvc.yaml\\n  11. create split _data.py to split data and add the split_ data stage to dvc.yaml\\n  12. create train _and_ evaluate.py to train & evaluate model and add the train _and_ evaluate stage to dvc.yaml\\n  13. Tox `bash tox tox -r #for rebuilding use this else only tox `\\n  14. pytest `bash pytest -v `\\n  15. setup command `bash pip install -e . `\\n  16. build your own package `bash python setup.py sdist bdist_wheel `\\n  17. create the need webapp structure\\n  18. Create app.py \\n  19. Add github action workflow\\n  20. Create an articat folder\\n  21. MLflow server command `bash mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 -p 1234 `\\n22. StockPrediction\\nupdated_at -> 2023-08-24\\ncreated_at -> 2023-08-24\\npushed_at -> 2023-08-24\\nJupyter Notebook,'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='# StockPrediction\\nThis notebook has been executed on kaggle. To view the notebook and run it,\\nvisit : https://www.kaggle.com/code/lokesh5489/stock\\n23. student_performance\\nupdated_at -> 2025-03-02\\ncreated_at -> 2025-02-28\\npushed_at -> 2025-03-02\\nJupyter Notebook, Python, HTML,\\n# End to End ML Project (student_performance)\\n## workflow\\n  1. created setup.py, template.py and requirements.txt\\n  2. Created custom exception in exception.py and logger.py and a function to read yaml\\n  3. created EDA and Model trainer in research\\n  4. Add new config.yaml file and done data_ingestion\\n  5. Completed data transformation\\n  6. Model Training\\n  7. Created the prediction pipeline and the Flask app\\n  8. Created .ebextensions for deployment\\n  9. create .github actions\\n  10. create ECR repo and save the repo uri \\n  11. create a ec2 instance \\n  12. Create a iam user and save a access key\\n  13. In github, in settings under action/runner create a new runner. a. while running the cmds in order when asked for runner group keep it default but for runner name use self-hosted\\n  14. Create Secret keys in github which are in workflow\\n## Docker Setup In EC2 commands to be Executed'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='# optinal\\n  1. sudo apt-get update -y\\n  2. sudo apt-get upgrade\\n# required\\n  1. curl -fsSL https://get.docker.com -o get-docker.sh\\n  2. sudo sh get-docker.sh\\n  3. sudo usermod -aG docker ubuntu\\n  4. newgrp docker \\n## To run\\n  1. Create a virtual env and activate it `bash conda create -p env python y conda activate /path/to/env `\\n  2. Install requirements.txt `bash pip install -r requirements.txt `\\n24. Telecom_Churn_Prediction\\nupdated_at -> 2023-08-23\\ncreated_at -> 2023-08-23\\npushed_at -> 2024-02-14\\nJupyter Notebook,\\n# Telecom _Churn_ Prediction\\nThis notebook has been executed on kaggle for utilising its GPU feature. To\\nview the dataset, visit https://www.kaggle.com/code/lokesh5489/telecomm-\\nchurn/data To visit the notebook, visit\\nhttps://www.kaggle.com/code/lokesh5489/telecomm-churn/notebook\\n25. Text-Summarizer\\nupdated_at -> 2024-09-20\\ncreated_at -> 2024-08-15\\npushed_at -> 2024-09-20\\nJupyter Notebook, Python,\\n# End-to-End Text-Summarizer'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='## Workflows\\n  1. Update config.yaml\\n  2. Update params.yaml\\n  3. Update entity\\n  4. Update configuration manager in src/config\\n  5. Update components\\n  6. Update pipeline\\n  7. Update main.py\\n  8. Update app.py\\n26. Video_Web\\nupdated_at -> 2023-08-26\\ncreated_at -> 2023-08-26\\npushed_at -> 2023-08-31\\nJavaScript, CSS, HTML,\\n# LuffyMeet - A Video chat web application\\n###  Intro\\nThis is a webRTC based video chatting web application. The users can register\\nand get logged in to their user page. They can either create a new room or use\\ntheir previous rooms to join a call and chat. This implementation also\\nconsists of a real time chat and file share functionality.\\n# Deployed link\\n[noobmeet.herokuapp.com](https://noobmeet.herokuapp.com/)  \\n(Calls and chats are functional, but becasue of NO \"Access-Control-Allow-\\nOrigin\" on icon package script, fonts-awesome is blocked. So u won\\'t be able\\nto access any incall-features in the deployement. To use all the features,\\nfollow the steps in \"Getting Started\" and run it on a local machine, no\\nproblems should arise, but depending on the time, the script source may\\nchange, so u have to manually change the source in local machine)\\n# Features and Functionalities\\n  * Unlimited number of rooms \\n  * Unlimited duration calls \\n  * Multiple participants \\n  * Copy the link and share it with your friends \\n  * Toggling your audio/video stream \\n  * Mute and Hide Everyone'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='* Chat and File Share in real-time \\n  * Chat before and after the meeting \\n  * Send individual messages to the participants online in the room \\n  * Screen Sharing \\n  * Recording your stream, audio and video \\n  * Full Screen Mode on double click on the video \\n# ScreenShots\\n  * Login and Register page\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010820-11f827e8-895b-4274-b568-d997184240d3.png)\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010848-5471e60c-3261-49a3-9195-a380e0085db2.png)\\n  * Main Page\\n    * Contains all the past rooms in sorted according to the time of thier last update.\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010985-a214ddf7-6ddb-42d7-8a66-a71e065fa1c4.png)\\n    * Creating a new room\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011685-74a216e6-e020-4a17-906c-763c3d3c6682.png)\\n  * Call page\\n    * Call Page (videos are just turned off, it is functional)\\n![Screenshot \\\\(1074\\\\)](https://user-\\nimages.githubusercontent.com/72460532/177011233-1c00902c-9047-41ef-9f7f-f220a4bd9d9f.png)\\n    * Automatic pop up of chat box upon recieving an message. The chat box is draggable (hold the body instead of the bar). \\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011335-d0d75476-c963-435b-9c38-a67f9439aab1.png)\\n  * Chat page - Contains all the chats form previous meetings.\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011551-15db10a1-ece1-4b9e-af36-86899d273424.png)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='A more detailed description and ScreenShots will be uploaded soon. (With video\\nturned on too :-)).\\n# Demo (Not functional as now)\\n  * Open https://noobmeet.herokuapp.com/\\n  * Create an account\\n  * Set a meeting name to create your room\\n  * Click on call button and give access to camera and microphone to join call\\n  * Click on chat button to chat before the meeting starts and as well as after the meeting ends\\n  * Share the room link for others to join \\n# Getting Started\\n  * You need to have Node.js installed\\n  * clone this repo\\n    * git clone https://github.com/fivar-rox/Video_chat.git\\n    * cd video-call\\n  * Install dependencies\\n    * npm init\\n    * npm install\\n  * Start the server \\n    * npm start\\n  * open http://localhost:4000 in your browser\\n# Tech Stack\\n  * Node.js \\n  * Web RTC \\n  * Socket.io \\n  * ngrok and stun-turn server\\n  * Firebase - for database \\n  * Heroku - for hosting'),\n",
       " Document(metadata={'producer': 'Microsoft® Excel® 2016', 'creator': 'Microsoft® Excel® 2016', 'creationdate': '2025-03-11T11:41:01+05:30', 'moddate': '2025-03-11T11:41:01+05:30', 'source': 'artifacts\\\\data_transformation\\\\Skills.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Skills\\nAgentic AI\\nGoogle Gemini\\nGenerative AI\\nLangChain\\nNatural Language Processing (NLP)\\nData Science\\nTensorFlow\\nDeep Learning\\nMachine Learning\\nPython (Programming Language)\\nObject-Oriented Programming (OOP)\\nDatasets\\nSQL\\nDjango\\npandas\\nNumPy\\nScikit-Learn\\nSoftware Development\\nData Structures\\nAlgorithms\\nMatplotlib\\nC++\\nApriori\\nReact.js\\nComputer networks\\nNode.js\\nEngineering\\nNetworking\\nCommunication\\nProject Management\\nyfinance')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"artifacts/data_transformation\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-11T05:05:33+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'artifacts\\\\data_transformation\\\\Profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"Contact\\n9842345676 (Mobile)\\nappsbotta@gmail.com\\nwww.linkedin.com/in/lokesh5489\\n(LinkedIn)\\nTop Skills\\nAgentic AI\\nGoogle Gemini\\nGenerative AI\\nLokesh Apparao Botta\\nData Science | Agentic AI | NLP | IIT Guwahati'24\\nNarsipatnam, Andhra Pradesh, India\\nSummary\\nI am graduate in electronics and communications engineering at IIT\\nGuwahati, one of the premier institutes of technology in India. I have\\na strong academic background and Deep Learning and Machine\\nLearning skills. My areas of interest are machine learning, deep\\nneural networks and computer networks. I aspire to pursue a career\\nin these domains, and contribute to the advancement of science\\nand technology. Along with this i have a working experience as a\\nresearch intern on resource allocation for D2D communication.\\nExperience\\nIndian Institute of Technology, Guwahati\\nResearch Intern\\nMay 2023\\xa0-\\xa0March 2024\\xa0(11 months)\\nGuwahati, Assam, India\\n-Engaged in research and projects alongside Professor Sukumar Nandi.\"),\n",
       " Document(metadata={'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-11T05:05:33+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'artifacts\\\\data_transformation\\\\Profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Guwahati, Assam, India\\n-Engaged in research and projects alongside Professor Sukumar Nandi.\\n-Developing a graph based algorithm for resource allocation for Device-to-\\nDevice communication for better Quality of Service for both cellular and D2D\\nusers.\\nEducation\\nIndian Institute of Technology, Guwahati\\nBachelor of Technology - B.Tech,\\xa0Electrical, Electronics and Communications\\nEngineering\\xa0·\\xa0(November 2020\\xa0-\\xa0July 2024)\\nSri Chaitanya College of Education\\nIntermediate\\xa0·\\xa0(2018\\xa0-\\xa02020)\\nSri Chaitanya College of Education\\n10th,SSC\\xa0·\\xa0(July 2017\\xa0-\\xa0March 2018)\\n\\xa0 Page 1 of 1'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='1. AgenticAi ---> https://github.com/appsbotta/AgenticAi\\n2. AI_Assistant ---> https://github.com/appsbotta/AI_Assistant\\n3. AWS_BEDROCK ---> https://github.com/appsbotta/AWS_BEDROCK\\n4. DataSets ---> https://github.com/appsbotta/DataSets\\n5. DL-End_to_End ---> https://github.com/appsbotta/DL-End_to_End\\n6. FaceDetection_React ---> https://github.com/appsbotta/FaceDetection_React\\n7. FitnessTracker ---> https://github.com/appsbotta/FitnessTracker\\n8. Gemini ---> https://github.com/appsbotta/Gemini\\n9. ImageCaption ---> https://github.com/appsbotta/ImageCaption\\n10. JosaaDataAnalysis ---> https://github.com/appsbotta/JosaaDataAnalysis\\n11. Kidney_diease_end_to_end --->\\nhttps://github.com/appsbotta/Kidney_diease_end_to_end\\n12. LangChain ---> https://github.com/appsbotta/LangChain\\n13. Llamaindex ---> https://github.com/appsbotta/Llamaindex\\n14. Market_Basket ---> https://github.com/appsbotta/Market_Basket\\n15. Networks ---> https://github.com/appsbotta/Networks'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='15. Networks ---> https://github.com/appsbotta/Networks\\n16. OlX_CLONE ---> https://github.com/appsbotta/OlX_CLONE\\n17. Portifolio ---> https://github.com/appsbotta/Portifolio\\n18. Q-A_ChatBot_with_Implementation --->\\nhttps://github.com/appsbotta/Q-A_ChatBot_with_Implementation\\n19. Real-Time-chatting ---> https://github.com/appsbotta/Real-Time-chatting\\n20. SentimentAnalysis ---> https://github.com/appsbotta/SentimentAnalysis\\n21. SimpleDVC ---> https://github.com/appsbotta/SimpleDVC\\n22. StockPrediction ---> https://github.com/appsbotta/StockPrediction\\n23. student_performance --->\\nhttps://github.com/appsbotta/student_performance\\n24. Telecom_Churn_Prediction --->\\nhttps://github.com/appsbotta/Telecom_Churn_Prediction\\n25. Text-Summarizer ---> https://github.com/appsbotta/Text-Summarizer\\n26. Video_Web ---> https://github.com/appsbotta/Video_Web\\n1. AgenticAi\\nupdated_at -> 2025-03-10\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-10'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Python,\\nNo Readme FIle\\n2. AI_Assistant\\nupdated_at -> 2025-03-10\\ncreated_at -> 2025-03-09\\npushed_at -> 2025-03-10\\nJupyter Notebook, Python,\\n# AI_Assistant\\n3. AWS_BEDROCK\\nupdated_at -> 2025-03-07\\ncreated_at -> 2025-03-06\\npushed_at -> 2025-03-07\\nPython,\\n# AWS_BEDROCK\\n# to run this\\n## 1\\\\. Git clone the repo\\n## 2\\\\. Install requirements.txt\\n## 3\\\\. Create an IAM user with Administrtor access\\n## 4\\\\. Configure AWS in cli\\n`bash aws configure `\\n4. DataSets\\nupdated_at -> 2025-03-03\\ncreated_at -> 2023-08-27\\npushed_at -> 2025-03-03\\nJupyter Notebook,\\nNo Readme FIle\\n5. DL-End_to_End'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='updated_at -> 2025-03-05\\ncreated_at -> 2025-03-02\\npushed_at -> 2025-03-05\\nJupyter Notebook, Python, HTML,\\n# DL-End _to_ End\\n## Workflow\\n  1. Update config.yaml in config dir\\n  2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity\\n  5. Update the configuration manager in src config\\n  6. Update the components\\n  7. Update the pipeline\\n  8. Update the main.py\\n  9. Update the dvc.yaml\\n# To run the repo\\n## steps\\nClone the repository `bash https://github.com/appsbotta/DL-End_to_End `\\n## step - 01 Create a conda env after opening the repo\\n`bash conda create -n ckn python=3.13 -y ` `bash conda activate ckn `\\n## step - 02 Install Requirements.txt\\n`bash pip install -r requirements.txt `\\n## step - 03 Run app.py\\n`bash python app.py `\\n## DVC cmd\\n  * dvc init\\n  * dvc repro\\n  * dvc dag\\n# AWS CI-CD Deployment using Github Actions'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='## 1\\\\. Login to AWS Console\\n## 2\\\\. Create IAM user for deployment\\n```bash\\n# give these access to IAM user\\n  1. EC2 access -> this is virtual machine\\n  2. ECR: Elastic COntainer registery to save your docker image in aws\\n# About deployment\\n  1. build docker image of the source code\\n  2. Push your docker image to ECR\\n  3. Launch your EC2 instance\\n  4. Pull your image from ECR to EC2\\n  5. Launch your docker image in EC2\\n# policy\\n  1. AmazonEC2ContainerRegistryFullAccess\\n  2. AmazonEC2FullAccess\\n```\\n## 3\\\\. Create ECR repo to store /dave docker image\\n`bash copy the repo uri ex : 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com/chicken `\\n## 4\\\\. Create an EC2 machine Instance(Ubuntu)\\n## 5\\\\. Connect EC2 instance and install docker\\n```bash\\n# optinal\\nsudo apt-get update -y'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content=\"sudo apt-get upgrade\\n# required\\ncurl -fsSL https://get.docker.com -o get-docker.sh\\nsudo sh get-docker.sh\\nsudo usermod -aG docker ubuntu\\nnewgrp docker ```\\n## 6\\\\. create a self runner in github repo settings\\n`bash in settings under action/runner create a new runner. a. while running\\nthe cmds in order when asked for runner group keep it default but for runner\\nname use self-hosted `\\n## 7\\\\. Setup github secrets\\n```bash AWS _ACCESS_ KEY_ID=\\nAWS _SECRET_ ACCESS_KEY=\\nAWS_REGION = us-east-1\\nAWS _ECR_ LOGIN_URI = ''(for example it will be 022499019910.dkr.ecr.us-\\neast-1.amazonaws.com)\\nECR _REPOSITORY_ NAME = '' (example chicken as last part of ECR uri)\\n6. FaceDetection_React\\nupdated_at -> 2023-08-19\\ncreated_at -> 2023-08-19\\npushed_at -> 2023-08-19\\nJavaScript, HTML, CSS,\\n# facedetection\\n7. FitnessTracker\\nupdated_at -> 2023-08-27\\ncreated_at -> 2023-08-27\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='pushed_at -> 2023-08-27\\nJavaScript, HTML, CSS,\\n# Fitness Tracker\\n## Description\\nThis is a Fitness Tracker built using MERN stack. It facilitates one to add\\nusers and record their date and duration of performing particular fitness\\nactivity.Activity can also be deleted. This web application is built using\\nMongoDB,Express,Nodejs,React and Material UI. Images used in this application\\nare being taken from [Unsplash website](https://unsplash.com).\\n## Demo\\n■ [Link](https://serene-volhard-1843cb.netlify.app/)\\nBackend Heroku Link-https://fitness-tracker-mern.herokuapp.com/\\n[Users](https://fitness-tracker-mern.herokuapp.com/users)\\n[Exercises](https://fitness-tracker-mern.herokuapp.com/exercises)\\n## Screenshots\\nHomepage ![fitkit_home](https://user-\\nimages.githubusercontent.com/4997491/117522296-20021d80-afd0-11eb-8079-922a4d51925a.JPG)\\nCreate user ![fitkit_user](https://user-\\nimages.githubusercontent.com/4997491/117525054-3911cb00-afde-11eb-97b5-a468f17935de.JPG)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='images.githubusercontent.com/4997491/117525054-3911cb00-afde-11eb-97b5-a468f17935de.JPG)\\nRecord Fitness Activity ![fitkit_exercise](https://user-\\nimages.githubusercontent.com/4997491/117525061-3d3de880-afde-11eb-9201-acdb5b804f40.JPG)\\nFitness Activity Dashboard ![fitkit_dashboard](https://user-\\nimages.githubusercontent.com/4997491/117525087-652d4c00-afde-11eb-92d4-0fda6f5c5d03.JPG)\\n404 Error page ![error](https://user-\\nimages.githubusercontent.com/4997491/117525227-dcfb7680-afde-11eb-9434-a8f93c5b76d7.JPG)\\n## Available Scripts\\nIn the project directory, you can run:\\n`npm start`'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='Runs the app in the development mode.  \\nOpen <http://localhost:3000> to view it in the browser.\\n8. Gemini\\nupdated_at -> 2025-02-16\\ncreated_at -> 2025-01-16\\npushed_at -> 2025-02-16\\nPython,\\nNo Readme FIle\\n9. ImageCaption\\nupdated_at -> 2024-08-04\\ncreated_at -> 2023-06-20\\npushed_at -> 2024-08-04\\nPython, HTML,\\n# image-caption-generator\\n## Deployed Link\\n[Image Caption](https://image-caption-gen-deploy-1.streamlit.app/)\\n# Home page\\n![Screenshot \\\\(82\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/33837fd6-2e5a-4329-9038-5034fecc4e11)\\n# Upon selecting caption predictor\\n![Screenshot \\\\(470\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/20c43357-01e9-4c5d-94ad-6182d370a625)\\n# Sample Images\\n![Screenshot \\\\(28\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/f6fe65e4-964e-48f0-98ef-703cdd872750) ![Screenshot\\n\\\\(27\\\\)](https://github.com/Vidya132/image-caption-\\ngenerator/assets/95306028/492b7abd-e277-4e06-b2d0-a8ffef509597)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='generator/assets/95306028/492b7abd-e277-4e06-b2d0-a8ffef509597)\\n10. JosaaDataAnalysis\\nupdated_at -> 2023-08-28'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='created_at -> 2023-08-28\\npushed_at -> 2023-08-28\\nHTML, Python, CSS,\\n# JOSAA-data-analysis\\nThis project aims to create a portal that allows users to explore the seat\\nallotment statistics of JOSAA (Joint Seat Allocation Authority) from 2016 to\\n2022. The project involves web scraping data from the JOSAA website,\\nperforming data cleaning and analysis, and presenting the insights through\\nvisualizations on a website frontend. The website presents the findings\\nthrough interactive charts and tables, enabling users to gain valuable\\ninsights into the trends and patterns of seat allotments over the years.\\n## Tech Stack/Frameworks:\\n  * Frontend: HTML, CSS, JavaScript\\n  * Backend: Django\\n  * Database: SQLite\\n  * Data Scraping: Beautiful Soup\\n  * Data Cleaning: NumPy, Pandas\\n  * Data Visualization: Chart.js\\n## Command to run the project locally\\n  * python manage.py runserver\\n  * Then run the server locally on port 8000\\n## Home page'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content=\"* python manage.py runserver\\n  * Then run the server locally on port 8000\\n## Home page\\nThe home page of the JOSAA Analysis portal provides a user-friendly interface\\nto explore and analyze the JOSAA seat allotment statistics. It offers several\\ninsightful sections to delve into the data and gain valuable information.\\nHere's a breakdown of each section: \\\\- View Institute wise cut-off \\\\- Analyze\\ninstitute wise cut-off trends \\\\- Roundwise cut-off Analysis\\n  * ![home](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/1d752e47-ef2d-4b4c-bf66-77fc8817c09d)\\n## View Institute wise cut-off list page\\n  * Upon clicking this option, a list of all IITs is shown along with their NIRF ranking(2023), Location and their established year. So the users can select specific institutes and explore the corresponding cut-off data.\\n  * ![iit list](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/f4f562e1-c93b-48a5-a114-4c321b296fe6)\\n    * ### For each IIT\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='* ### For each IIT \\n      * By selecting a specific IIT, users can explore the trends and changes in the cut-off ranks throughout the various rounds of the seat allocation process over the years 2016 to 2022.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='* It can further narrow down their search by choosing specific programs offered by the selected institutes along with seat type and gender.\\n      * This feature helps users understand the cut-off trends for different programs in their preferred institutes.\\n      * ![first graph](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/5c411f78-ea17-4218-a0d6-9a8740ad78b5)\\n## Analyze institute wise cut-off trends\\n  * Institute trends highlight the trends of various programs offered by a particular institute over the years. This helps understand the popularity and perception of programs offered by the institute, and thus helps understand the demand for a particular program in the institute during the counselling process.\\n  * ![Screenshot \\\\(20\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/45d3feab-6bf4-4611-91fb-91580b6a90ce)\\n  * Upon sleecting the gender, branch and institute, user will get the cut-off analysis for each branch present at that IIT.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='* ![Screenshot \\\\(22\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/1db046fe-c44c-4627-8282-c8143bf0a88a)\\n## Roundwise cut-off Analysis\\n  * Round trends highlight the general trend of closing ranks throughout the rounds of the counselling process. This helps understand the likely range of changes to the closing ranks throught the counselling proces.\\n  * Users can get the round wise analysis at each IIT by selecting the gender,seat type and branch. \\n  * User can change the college name by clicking on \"Go to iit-list\" buttion.\\n  * ![Screenshot \\\\(25\\\\)](https://github.com/Vidya132/JOSAA-data-analysis/assets/95306028/3206dd19-d8fb-4e88-ace9-84a743db723e)\\n11. Kidney_diease_end_to_end\\nupdated_at -> 2025-03-05\\ncreated_at -> 2025-03-05\\npushed_at -> 2025-03-05\\nPython, Jupyter Notebook,\\n# Kidney _diease_ end _to_ end\\n## Workflow\\n  1. Update config.yaml in config dir\\n  2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='2. Update secrets.yaml [optional]\\n  3. Update params.yaml\\n  4. Update the entity\\n  5. Update the configuration manager in src config\\n  6. Update the components\\n  7. Update the pipeline\\n  8. Update the main.py\\n  9. Update the dvc.yaml\\n# To run the repo\\n## steps'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Clone the repository `bash\\nhttps://github.com/appsbotta/Kidney_diease_end_to_end `\\n## step - 01 Create a conda env after opening the repo\\n`bash conda create -n ckn python=3.8 -y ` `bash conda activate ckn `\\n## step - 02 Install Requirements.txt\\n`bash pip install -r requirements.txt `\\n## step - 03 Run app.py\\n`bash python app.py `\\n## DVC cmd\\n  * dvc init\\n  * dvc repro\\n  * dvc dag\\n# AWS CI-CD Deployment using Github Actions\\n## 1\\\\. Login to AWS Console\\n## 2\\\\. Create IAM user for deployment\\n```bash\\n# give these access to IAM user\\n  1. EC2 access -> this is virtual machine\\n  2. ECR: Elastic COntainer registery to save your docker image in aws\\n# About deployment\\n  1. build docker image of the source code\\n  2. Push your docker image to ECR\\n  3. Launch your EC2 instance\\n  4. Pull your image from ECR to EC2\\n  5. Launch your docker image in EC2'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='# policy\\n  1. AmazonEC2ContainerRegistryFullAccess\\n  2. AmazonEC2FullAccess\\n```\\n12. LangChain\\nupdated_at -> 2025-03-09\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-09\\nJupyter Notebook, Python,\\n# LangChain Projects\\nThis repository showcases various applications and experiments utilizing the\\nLangChain framework, which is designed for building applications powered by\\nlarge language models (LLMs).\\n## Table of Contents\\n  * Projects\\n    * LLMAPP\\n    * PdfQuery\\n    * ChatMultipleDocs\\n    * TextSummarization\\n    * Celebrity.py\\n  * Installation\\n  * Usage\\n  * Contributing\\n  * License\\n## Projects\\n### LLMAPP\\nA project demonstrating the integration of LangChain with large language\\nmodels to build intelligent applications.\\n### PdfQuery\\nAn application that utilizes LangChain to extract and process information from\\nPDF documents.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='### ChatMultipleDocs\\nA chatbot implementation capable of interacting with multiple documents,\\npowered by LangChain.\\n### TextSummarization\\nA tool that leverages LangChain to provide text summarization capabilities.\\n### Celebrity.py\\nA script that explores web to get the data related to a Celebrity given his\\nname and any 5 major events happend around his DOB\\n## Installation\\n  1. **Clone the repository:**\\n```bash git clone https://github.com/appsbotta/LangChain.git cd LangChain\\n  2. **Create a virtual environment:**\\n```bash conda create -p myenv python conda activate path/to/myenv\\n  3. **Install the required dependencies:**\\n```bash pip install -r requirements.txt\\n  4. **Create a`.env` file in the root of the repository:**\\nAdd your API keys to the .env file ```bash touch .env\\n## Usage\\nEach project is contained within its respective directory. Navigate to the\\ndesired project folder and follow the instructions provided above\\n13. Llamaindex\\nupdated_at -> 2025-01-17\\ncreated_at -> 2025-01-17'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='13. Llamaindex\\nupdated_at -> 2025-01-17\\ncreated_at -> 2025-01-17\\npushed_at -> 2025-01-17\\nJupyter Notebook,\\n# Llamaindex'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='14. Market_Basket\\nupdated_at -> 2023-08-15\\ncreated_at -> 2023-08-15\\npushed_at -> 2023-08-15\\nJupyter Notebook,\\n# Market_Basket\\nThis notebook has been executed on kaggle. To view the notebook and run it,\\nhttps://www.kaggle.com/code/lokesh5489/market-basket To view the concerned\\ndataset, visit https://www.kaggle.com/code/lokesh5489/market-basket/data\\nUsed Aprior algorithm to find relation between two products\\n15. Networks\\nupdated_at -> 2024-05-25\\ncreated_at -> 2024-05-23\\npushed_at -> 2024-05-25\\nC++, HTML, C,\\n# # The Network Simulator, Version 3\\n## Changes that we made\\n### 1.lte-ue-mac.cc\\n  * added one pre-defined table for type-0 allocation at line 52\\n  * Added Pf in line 404\\n  * Added function to get rbg size at line 503\\n  * line 2482 for grant printing in random allocation\\n  * line 2532 code for PF allocation, CQI calculation,mcs from cqi\\n  * added function at line 2715\\n### 2.lte-sl-out-of-covrg-comm.cc\\n  * At line 268 added code to print from rrc'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='### 2.lte-sl-out-of-covrg-comm.cc\\n  * At line 268 added code to print from rrc\\n  * At line 412 added code to print from mac\\n16. OlX_CLONE\\nupdated_at -> 2023-08-26\\ncreated_at -> 2023-08-26'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='pushed_at -> 2023-08-26\\nJavaScript, CSS, HTML,\\n# OLX _clone_ IITG\\n## About the website\\nUsers are directed to login page on opening the website. Users have to login\\nusing their Outlook Id. Later user will be directed to home page of the\\nwebsite where two buttons will be displayed. Here user can choose whether to\\nbuy or sell and based on that available categories will be diplayed. Profile\\nshows logged in users profile. Logout button logs out the user.\\n## Environment Setup Locally\\n  * Install React\\n  * Use npm install in both Backend and Client \\n  * ...\\n## Tech Stack Used\\n  * React\\n  * MongoDB\\n  * Express\\n  * Node\\n  * HTML\\n  * CSS\\n  * Javascript\\n## Key Features\\n  * User can login and logout using Outlook Id\\n  * Functionality to buy and sell\\n  * Seller Profile Containing all the products he want to sell and stats of every deal he made, like people who are interested with listed money'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='* Search functionality for buyer and filter method to get his wanted product within the desired price range\\n  * Product details along with buyer details\\n  * Option to agree to listed deal or to reach out seller for some negotiation for a particular product\\n  * Marks product as sold if deal is made\\n## Login page\\n  * The web application is authenticated and user must sign in using Outlook Id. ![image](https://user-images.githubusercontent.com/95306028/177820740-33b49604-b1b1-4285-bc41-9b9a5bf5b1ce.png)\\n## Home page\\n  * After login the home page will be displayed, where user can choose to buy or sell items. ![image](https://user-images.githubusercontent.com/95306028/177821314-90828c2c-6ef6-4644-9729-df123930b31c.png)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content=\"## Buyer Homepage\\n  * After clicking **Buy** all categories which are available are displayed. User can also search for required item in search bar.\\n  * ![image](https://user-images.githubusercontent.com/95306028/177822651-b505f91e-2f11-4826-951c-e6e6e0560d97.png)\\n17. Portifolio\\nupdated_at -> 2025-03-04\\ncreated_at -> 2025-01-18\\npushed_at -> 2025-03-04\\nJavaScript, HTML, CSS,\\n# Portifolio\\n18. Q-A_ChatBot_with_Implementation\\nupdated_at -> 2025-01-08\\ncreated_at -> 2025-01-07\\npushed_at -> 2025-01-08\\nPython,\\n* * *\\ntitle: LangChain ChatBot emoji: ■ colorFrom: gray colorTo: blue sdk: streamlit\\nsdk _version: 1.41.1 app_ file: app.py\\n## pinned: false\\n19. Real-Time-chatting\\nupdated_at -> 2022-06-30\\ncreated_at -> 2022-06-30\\npushed_at -> 2022-07-07\\nHTML, CSS, JavaScript,\\n# Real-Time-chatting\\nUser are directed to opening page where ther need to click 'Enter ChatRoom',\\nUpon Clicking it users are redirected to a page where they need to enter then\"),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Upon Clicking it users are redirected to a page where they need to enter then\\nroom,upon that the user will enter into chat room where he can chat with his\\nother buddies!!\\n## Enviroment Setup Locally\\n  * Install Web Sockets and Expressjs'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='* Run the server and Connect to Port 500\\n## Tech Stack Used\\n  * HTML\\n  * JavaScript\\n  * Css\\n  * WebSockets\\n## Opening Page\\n  * This the Page we get when we Open localhost in port 5000\\n![OPENING](https://user-\\nimages.githubusercontent.com/75985363/177767489-6a32f337-8b3c-4281-a281-1d58c24ae3e0.jpg)\\n## Name Entry\\n  * User need to enter his name ![entering name](https://user-images.githubusercontent.com/75985363/177767789-5137808d-a34a-4bed-b415-b3ab67a61fca.jpg)\\n## Chat Room\\n  * After entering name User redirects to this page ![chat room](https://user-images.githubusercontent.com/75985363/177767981-0893a5eb-7aa2-4421-8bd1-da9592eeb46d.jpg)\\n20. SentimentAnalysis\\nupdated_at -> 2024-11-09\\ncreated_at -> 2024-11-09\\npushed_at -> 2024-11-09\\n# SentimentAnalysis\\n21. SimpleDVC\\nupdated_at -> 2025-02-24\\ncreated_at -> 2025-02-21\\npushed_at -> 2025-02-24\\nPython, Jupyter Notebook, HTML,\\n# WorkFlow of the Project\\n  1. Create env `bash conda create -p myenv python -y `'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='# WorkFlow of the Project\\n  1. Create env `bash conda create -p myenv python -y `\\n  2. activate env `bash conda activate (path to myenv) `'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='3. Create Requirements.txt\\n  4. Install requirements.txt `bash pip install -r requirements.txt `\\n  5. Initilize git `bash git init `\\n  6. Initilize DVC `bash dvc init `\\n  7. Add data to DVC `bash dvc add data_given/winequality.csv `\\n  8. `bash git add . && git commit -m \"first commit\" git push origin main `\\n  9. Create params.yaml\\n  10. create get _data.py and load_ data.py to get data and add the load_data stage to dvc.yaml\\n  11. create split _data.py to split data and add the split_ data stage to dvc.yaml\\n  12. create train _and_ evaluate.py to train & evaluate model and add the train _and_ evaluate stage to dvc.yaml\\n  13. Tox `bash tox tox -r #for rebuilding use this else only tox `\\n  14. pytest `bash pytest -v `\\n  15. setup command `bash pip install -e . `\\n  16. build your own package `bash python setup.py sdist bdist_wheel `\\n  17. create the need webapp structure\\n  18. Create app.py \\n  19. Add github action workflow\\n  20. Create an articat folder'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='18. Create app.py \\n  19. Add github action workflow\\n  20. Create an articat folder\\n  21. MLflow server command `bash mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 -p 1234 `\\n22. StockPrediction\\nupdated_at -> 2023-08-24\\ncreated_at -> 2023-08-24\\npushed_at -> 2023-08-24\\nJupyter Notebook,'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='# StockPrediction\\nThis notebook has been executed on kaggle. To view the notebook and run it,\\nvisit : https://www.kaggle.com/code/lokesh5489/stock\\n23. student_performance\\nupdated_at -> 2025-03-02\\ncreated_at -> 2025-02-28\\npushed_at -> 2025-03-02\\nJupyter Notebook, Python, HTML,\\n# End to End ML Project (student_performance)\\n## workflow\\n  1. created setup.py, template.py and requirements.txt\\n  2. Created custom exception in exception.py and logger.py and a function to read yaml\\n  3. created EDA and Model trainer in research\\n  4. Add new config.yaml file and done data_ingestion\\n  5. Completed data transformation\\n  6. Model Training\\n  7. Created the prediction pipeline and the Flask app\\n  8. Created .ebextensions for deployment\\n  9. create .github actions\\n  10. create ECR repo and save the repo uri \\n  11. create a ec2 instance \\n  12. Create a iam user and save a access key'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='11. create a ec2 instance \\n  12. Create a iam user and save a access key\\n  13. In github, in settings under action/runner create a new runner. a. while running the cmds in order when asked for runner group keep it default but for runner name use self-hosted\\n  14. Create Secret keys in github which are in workflow\\n## Docker Setup In EC2 commands to be Executed'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='# optinal\\n  1. sudo apt-get update -y\\n  2. sudo apt-get upgrade\\n# required\\n  1. curl -fsSL https://get.docker.com -o get-docker.sh\\n  2. sudo sh get-docker.sh\\n  3. sudo usermod -aG docker ubuntu\\n  4. newgrp docker \\n## To run\\n  1. Create a virtual env and activate it `bash conda create -p env python y conda activate /path/to/env `\\n  2. Install requirements.txt `bash pip install -r requirements.txt `\\n24. Telecom_Churn_Prediction\\nupdated_at -> 2023-08-23\\ncreated_at -> 2023-08-23\\npushed_at -> 2024-02-14\\nJupyter Notebook,\\n# Telecom _Churn_ Prediction\\nThis notebook has been executed on kaggle for utilising its GPU feature. To\\nview the dataset, visit https://www.kaggle.com/code/lokesh5489/telecomm-\\nchurn/data To visit the notebook, visit\\nhttps://www.kaggle.com/code/lokesh5489/telecomm-churn/notebook\\n25. Text-Summarizer\\nupdated_at -> 2024-09-20\\ncreated_at -> 2024-08-15\\npushed_at -> 2024-09-20\\nJupyter Notebook, Python,\\n# End-to-End Text-Summarizer'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='## Workflows\\n  1. Update config.yaml\\n  2. Update params.yaml\\n  3. Update entity\\n  4. Update configuration manager in src/config\\n  5. Update components\\n  6. Update pipeline\\n  7. Update main.py\\n  8. Update app.py\\n26. Video_Web\\nupdated_at -> 2023-08-26\\ncreated_at -> 2023-08-26\\npushed_at -> 2023-08-31\\nJavaScript, CSS, HTML,\\n# LuffyMeet - A Video chat web application\\n###  Intro\\nThis is a webRTC based video chatting web application. The users can register\\nand get logged in to their user page. They can either create a new room or use\\ntheir previous rooms to join a call and chat. This implementation also\\nconsists of a real time chat and file share functionality.\\n# Deployed link\\n[noobmeet.herokuapp.com](https://noobmeet.herokuapp.com/)  \\n(Calls and chats are functional, but becasue of NO \"Access-Control-Allow-\\nOrigin\" on icon package script, fonts-awesome is blocked. So u won\\'t be able\\nto access any incall-features in the deployement. To use all the features,'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='to access any incall-features in the deployement. To use all the features,\\nfollow the steps in \"Getting Started\" and run it on a local machine, no\\nproblems should arise, but depending on the time, the script source may\\nchange, so u have to manually change the source in local machine)\\n# Features and Functionalities\\n  * Unlimited number of rooms \\n  * Unlimited duration calls \\n  * Multiple participants \\n  * Copy the link and share it with your friends \\n  * Toggling your audio/video stream \\n  * Mute and Hide Everyone'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='* Chat and File Share in real-time \\n  * Chat before and after the meeting \\n  * Send individual messages to the participants online in the room \\n  * Screen Sharing \\n  * Recording your stream, audio and video \\n  * Full Screen Mode on double click on the video \\n# ScreenShots\\n  * Login and Register page\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010820-11f827e8-895b-4274-b568-d997184240d3.png)\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010848-5471e60c-3261-49a3-9195-a380e0085db2.png)\\n  * Main Page\\n    * Contains all the past rooms in sorted according to the time of thier last update.\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177010985-a214ddf7-6ddb-42d7-8a66-a71e065fa1c4.png)\\n    * Creating a new room\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011685-74a216e6-e020-4a17-906c-763c3d3c6682.png)\\n  * Call page\\n    * Call Page (videos are just turned off, it is functional)\\n![Screenshot \\\\(1074\\\\)](https://user-'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='* Call Page (videos are just turned off, it is functional)\\n![Screenshot \\\\(1074\\\\)](https://user-\\nimages.githubusercontent.com/72460532/177011233-1c00902c-9047-41ef-9f7f-f220a4bd9d9f.png)\\n    * Automatic pop up of chat box upon recieving an message. The chat box is draggable (hold the body instead of the bar). \\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011335-d0d75476-c963-435b-9c38-a67f9439aab1.png)\\n  * Chat page - Contains all the chats form previous meetings.\\n![image](https://user-\\nimages.githubusercontent.com/72460532/177011551-15db10a1-ece1-4b9e-af36-86899d273424.png)'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='A more detailed description and ScreenShots will be uploaded soon. (With video\\nturned on too :-)).\\n# Demo (Not functional as now)\\n  * Open https://noobmeet.herokuapp.com/\\n  * Create an account\\n  * Set a meeting name to create your room\\n  * Click on call button and give access to camera and microphone to join call\\n  * Click on chat button to chat before the meeting starts and as well as after the meeting ends\\n  * Share the room link for others to join \\n# Getting Started\\n  * You need to have Node.js installed\\n  * clone this repo\\n    * git clone https://github.com/fivar-rox/Video_chat.git\\n    * cd video-call\\n  * Install dependencies\\n    * npm init\\n    * npm install\\n  * Start the server \\n    * npm start\\n  * open http://localhost:4000 in your browser\\n# Tech Stack\\n  * Node.js \\n  * Web RTC \\n  * Socket.io \\n  * ngrok and stun-turn server\\n  * Firebase - for database \\n  * Heroku - for hosting'),\n",
       " Document(metadata={'producer': 'Microsoft® Excel® 2016', 'creator': 'Microsoft® Excel® 2016', 'creationdate': '2025-03-11T11:41:01+05:30', 'moddate': '2025-03-11T11:41:01+05:30', 'source': 'artifacts\\\\data_transformation\\\\Skills.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Skills\\nAgentic AI\\nGoogle Gemini\\nGenerative AI\\nLangChain\\nNatural Language Processing (NLP)\\nData Science\\nTensorFlow\\nDeep Learning\\nMachine Learning\\nPython (Programming Language)\\nObject-Oriented Programming (OOP)\\nDatasets\\nSQL\\nDjango\\npandas\\nNumPy\\nScikit-Learn\\nSoftware Development\\nData Structures\\nAlgorithms\\nMatplotlib\\nC++\\nApriori\\nReact.js\\nComputer networks\\nNode.js\\nEngineering\\nNetworking\\nCommunication\\nProject Management\\nyfinance')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents=documents,embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='61019927-b972-4503-b2a8-06f1e84bd90d', metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='15. Networks ---> https://github.com/appsbotta/Networks\\n16. OlX_CLONE ---> https://github.com/appsbotta/OlX_CLONE\\n17. Portifolio ---> https://github.com/appsbotta/Portifolio\\n18. Q-A_ChatBot_with_Implementation --->\\nhttps://github.com/appsbotta/Q-A_ChatBot_with_Implementation\\n19. Real-Time-chatting ---> https://github.com/appsbotta/Real-Time-chatting\\n20. SentimentAnalysis ---> https://github.com/appsbotta/SentimentAnalysis\\n21. SimpleDVC ---> https://github.com/appsbotta/SimpleDVC\\n22. StockPrediction ---> https://github.com/appsbotta/StockPrediction\\n23. student_performance --->\\nhttps://github.com/appsbotta/student_performance\\n24. Telecom_Churn_Prediction --->\\nhttps://github.com/appsbotta/Telecom_Churn_Prediction\\n25. Text-Summarizer ---> https://github.com/appsbotta/Text-Summarizer\\n26. Video_Web ---> https://github.com/appsbotta/Video_Web\\n1. AgenticAi\\nupdated_at -> 2025-03-10\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-10'),\n",
       " Document(id='64fc49bd-9da8-4256-9c21-574c72c2cf93', metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='# policy\\n  1. AmazonEC2ContainerRegistryFullAccess\\n  2. AmazonEC2FullAccess\\n```\\n12. LangChain\\nupdated_at -> 2025-03-09\\ncreated_at -> 2025-01-13\\npushed_at -> 2025-03-09\\nJupyter Notebook, Python,\\n# LangChain Projects\\nThis repository showcases various applications and experiments utilizing the\\nLangChain framework, which is designed for building applications powered by\\nlarge language models (LLMs).\\n## Table of Contents\\n  * Projects\\n    * LLMAPP\\n    * PdfQuery\\n    * ChatMultipleDocs\\n    * TextSummarization\\n    * Celebrity.py\\n  * Installation\\n  * Usage\\n  * Contributing\\n  * License\\n## Projects\\n### LLMAPP\\nA project demonstrating the integration of LangChain with large language\\nmodels to build intelligent applications.\\n### PdfQuery\\nAn application that utilizes LangChain to extract and process information from\\nPDF documents.'),\n",
       " Document(id='8c163a96-89ed-471e-8e8c-fc6566f2fbeb', metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='1. AgenticAi ---> https://github.com/appsbotta/AgenticAi\\n2. AI_Assistant ---> https://github.com/appsbotta/AI_Assistant\\n3. AWS_BEDROCK ---> https://github.com/appsbotta/AWS_BEDROCK\\n4. DataSets ---> https://github.com/appsbotta/DataSets\\n5. DL-End_to_End ---> https://github.com/appsbotta/DL-End_to_End\\n6. FaceDetection_React ---> https://github.com/appsbotta/FaceDetection_React\\n7. FitnessTracker ---> https://github.com/appsbotta/FitnessTracker\\n8. Gemini ---> https://github.com/appsbotta/Gemini\\n9. ImageCaption ---> https://github.com/appsbotta/ImageCaption\\n10. JosaaDataAnalysis ---> https://github.com/appsbotta/JosaaDataAnalysis\\n11. Kidney_diease_end_to_end --->\\nhttps://github.com/appsbotta/Kidney_diease_end_to_end\\n12. LangChain ---> https://github.com/appsbotta/LangChain\\n13. Llamaindex ---> https://github.com/appsbotta/Llamaindex\\n14. Market_Basket ---> https://github.com/appsbotta/Market_Basket\\n15. Networks ---> https://github.com/appsbotta/Networks'),\n",
       " Document(id='0efa2754-3b6f-4685-9441-d67a363da6db', metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-03-10T23:48:22+05:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-03-10T23:48:22+05:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'artifacts\\\\data_transformation\\\\repo.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content=\"## Buyer Homepage\\n  * After clicking **Buy** all categories which are available are displayed. User can also search for required item in search bar.\\n  * ![image](https://user-images.githubusercontent.com/95306028/177822651-b505f91e-2f11-4826-951c-e6e6e0560d97.png)\\n17. Portifolio\\nupdated_at -> 2025-03-04\\ncreated_at -> 2025-01-18\\npushed_at -> 2025-03-04\\nJavaScript, HTML, CSS,\\n# Portifolio\\n18. Q-A_ChatBot_with_Implementation\\nupdated_at -> 2025-01-08\\ncreated_at -> 2025-01-07\\npushed_at -> 2025-01-08\\nPython,\\n* * *\\ntitle: LangChain ChatBot emoji: ■ colorFrom: gray colorTo: blue sdk: streamlit\\nsdk _version: 1.41.1 app_ file: app.py\\n## pinned: false\\n19. Real-Time-chatting\\nupdated_at -> 2022-06-30\\ncreated_at -> 2022-06-30\\npushed_at -> 2022-07-07\\nHTML, CSS, JavaScript,\\n# Real-Time-chatting\\nUser are directed to opening page where ther need to click 'Enter ChatRoom',\\nUpon Clicking it users are redirected to a page where they need to enter then\")]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"list all projects\"\n",
    "res = db.similarity_search(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "you are a personal AI assistant for lokesh. \n",
    "Your provided with information of lokesh like, his project repository details, his education, his achievemets and skill set.\n",
    "\n",
    "Answer the following question related to lokesh based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $999 if the user finds the answer helpful.\n",
    "if the context is not sufficient then reply with\n",
    "\"I am sorry i dont have access to that data yet\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "question:{input}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm,\n",
    "    prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000214FFAC1090>, search_kwargs={'k': 30})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 30})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000214FFAC1090>, search_kwargs={'k': 30}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nyou are a personal AI assistant for lokesh. \\nYour provided with information of lokesh like, his project repository details, his education, his achievemets and skill set.\\n\\nAnswer the following question related to lokesh based only on the provided context.\\nThink step by step before providing a detailed answer.\\nI will tip you $999 if the user finds the answer helpful.\\nif the context is not sufficient then reply with\\n\"I am sorry i dont have access to that data yet\"\\n\\n<context>\\n{context}\\n</context>\\nquestion:{input}\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000214FFAC0410>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000214FFAC0050>, root_client=<openai.OpenAI object at 0x00000214FFAC1310>, root_async_client=<openai.AsyncOpenAI object at 0x00000214FFAC02D0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain = create_retrieval_chain(retriever,document_chain)\n",
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Lokesh has experience as a Research Intern at the Indian Institute of Technology, Guwahati from May 2023 to March 2024, which totals to 11 months of experience. During this time, he was engaged in research and projects alongside Professor Sukumar Nandi.\n"
     ]
    }
   ],
   "source": [
    "res = retriever_chain.invoke({\"input\":\"does he have any experience and if he has , what is the duration\"})\n",
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 945554174,\n",
       " 'node_id': 'R_kgDOOFwC_g',\n",
       " 'name': 'AI_Assistant',\n",
       " 'full_name': 'appsbotta/AI_Assistant',\n",
       " 'private': False,\n",
       " 'owner': {'login': 'appsbotta',\n",
       "  'id': 75985363,\n",
       "  'node_id': 'MDQ6VXNlcjc1OTg1MzYz',\n",
       "  'avatar_url': 'https://avatars.githubusercontent.com/u/75985363?v=4',\n",
       "  'gravatar_id': '',\n",
       "  'url': 'https://api.github.com/users/appsbotta',\n",
       "  'html_url': 'https://github.com/appsbotta',\n",
       "  'followers_url': 'https://api.github.com/users/appsbotta/followers',\n",
       "  'following_url': 'https://api.github.com/users/appsbotta/following{/other_user}',\n",
       "  'gists_url': 'https://api.github.com/users/appsbotta/gists{/gist_id}',\n",
       "  'starred_url': 'https://api.github.com/users/appsbotta/starred{/owner}{/repo}',\n",
       "  'subscriptions_url': 'https://api.github.com/users/appsbotta/subscriptions',\n",
       "  'organizations_url': 'https://api.github.com/users/appsbotta/orgs',\n",
       "  'repos_url': 'https://api.github.com/users/appsbotta/repos',\n",
       "  'events_url': 'https://api.github.com/users/appsbotta/events{/privacy}',\n",
       "  'received_events_url': 'https://api.github.com/users/appsbotta/received_events',\n",
       "  'type': 'User',\n",
       "  'user_view_type': 'public',\n",
       "  'site_admin': False},\n",
       " 'html_url': 'https://github.com/appsbotta/AI_Assistant',\n",
       " 'description': None,\n",
       " 'fork': False,\n",
       " 'url': 'https://api.github.com/repos/appsbotta/AI_Assistant',\n",
       " 'forks_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/forks',\n",
       " 'keys_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/keys{/key_id}',\n",
       " 'collaborators_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/collaborators{/collaborator}',\n",
       " 'teams_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/teams',\n",
       " 'hooks_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/hooks',\n",
       " 'issue_events_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues/events{/number}',\n",
       " 'events_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/events',\n",
       " 'assignees_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/assignees{/user}',\n",
       " 'branches_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/branches{/branch}',\n",
       " 'tags_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/tags',\n",
       " 'blobs_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/blobs{/sha}',\n",
       " 'git_tags_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/tags{/sha}',\n",
       " 'git_refs_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/refs{/sha}',\n",
       " 'trees_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/trees{/sha}',\n",
       " 'statuses_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/statuses/{sha}',\n",
       " 'languages_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/languages',\n",
       " 'stargazers_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/stargazers',\n",
       " 'contributors_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/contributors',\n",
       " 'subscribers_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/subscribers',\n",
       " 'subscription_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/subscription',\n",
       " 'commits_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/commits{/sha}',\n",
       " 'git_commits_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/git/commits{/sha}',\n",
       " 'comments_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/comments{/number}',\n",
       " 'issue_comment_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues/comments{/number}',\n",
       " 'contents_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/contents/{+path}',\n",
       " 'compare_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/compare/{base}...{head}',\n",
       " 'merges_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/merges',\n",
       " 'archive_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/{archive_format}{/ref}',\n",
       " 'downloads_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/downloads',\n",
       " 'issues_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/issues{/number}',\n",
       " 'pulls_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/pulls{/number}',\n",
       " 'milestones_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/milestones{/number}',\n",
       " 'notifications_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/notifications{?since,all,participating}',\n",
       " 'labels_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/labels{/name}',\n",
       " 'releases_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/releases{/id}',\n",
       " 'deployments_url': 'https://api.github.com/repos/appsbotta/AI_Assistant/deployments',\n",
       " 'created_at': '2025-03-09T17:38:25Z',\n",
       " 'updated_at': '2025-03-11T03:40:40Z',\n",
       " 'pushed_at': '2025-03-11T03:40:37Z',\n",
       " 'git_url': 'git://github.com/appsbotta/AI_Assistant.git',\n",
       " 'ssh_url': 'git@github.com:appsbotta/AI_Assistant.git',\n",
       " 'clone_url': 'https://github.com/appsbotta/AI_Assistant.git',\n",
       " 'svn_url': 'https://github.com/appsbotta/AI_Assistant',\n",
       " 'homepage': None,\n",
       " 'size': 57,\n",
       " 'stargazers_count': 0,\n",
       " 'watchers_count': 0,\n",
       " 'language': 'Jupyter Notebook',\n",
       " 'has_issues': True,\n",
       " 'has_projects': True,\n",
       " 'has_downloads': True,\n",
       " 'has_wiki': True,\n",
       " 'has_pages': False,\n",
       " 'has_discussions': False,\n",
       " 'forks_count': 0,\n",
       " 'mirror_url': None,\n",
       " 'archived': False,\n",
       " 'disabled': False,\n",
       " 'open_issues_count': 0,\n",
       " 'license': {'key': 'mit',\n",
       "  'name': 'MIT License',\n",
       "  'spdx_id': 'MIT',\n",
       "  'url': 'https://api.github.com/licenses/mit',\n",
       "  'node_id': 'MDc6TGljZW5zZTEz'},\n",
       " 'allow_forking': True,\n",
       " 'is_template': False,\n",
       " 'web_commit_signoff_required': False,\n",
       " 'topics': [],\n",
       " 'visibility': 'public',\n",
       " 'forks': 0,\n",
       " 'open_issues': 0,\n",
       " 'watchers': 0,\n",
       " 'default_branch': 'main',\n",
       " 'permissions': {'admin': True,\n",
       "  'maintain': True,\n",
       "  'push': True,\n",
       "  'triage': True,\n",
       "  'pull': True}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "url = \"https://api.github.com/user/repos?per_page=100\"\n",
    "\n",
    "headers = {\"Authorization\": f\"token {TOKEN}\"}\n",
    "repos = requests.get(url, headers=headers).json()\n",
    "\n",
    "# for repo in repos:\n",
    "#     print(repo)\n",
    "repos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# TOKEN = os.getenv(\"TOKEN\")\n",
    "url = 'https://api.github.com/repos/appsbotta/ImageCaption/languages'\n",
    "\n",
    "headers = {\"Authorization\": f\"token {TOKEN}\"}\n",
    "lang = requests.get(url).json()\n",
    "\n",
    "# for repo in repos:\n",
    "#     print(repo)\n",
    "top_three_keys = sorted(lang, key=lang.get, reverse=True)[:3]\n",
    "top_three_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = f\"https://api.github.com/repos/appsbotta/LangChain/readme\"\n",
    "response = requests.get(readme)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Appsb\\\\Desktop\\\\AI_Assistant'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('AI_Assistant')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.linkedin.com/in/lokesh5489/', 'language': 'No language found.'}, page_content='\\n\\n\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'),temperature=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
